#from SAGAN import SAGAN
import argparse
from utils import *
import numpy as np
from ops import *
from tensorflow.contrib.data import prefetch_to_device, shuffle_and_repeat, batch_and_drop_remainder

"""parsing and configuration"""
def parse_args():
    desc = "Vessel Matching with Self-Attention"
    parser = argparse.ArgumentParser(description=desc)
    parser.add_argument('--phase', type=str, default='train', help='train or test ?')
    parser.add_argument('--epoch', type=int, default=10, help='The number of epochs to run')
    parser.add_argument('--iteration', type=int, default=100, help='The number of training iterations')
    parser.add_argument('--batch_size', type=int, default=1, help='The size of batch per gpu')
    parser.add_argument('--print_freq', type=int, default=500, help='The number of image_print_freqy')
    parser.add_argument('--save_freq', type=int, default=2000, help='The number of ckpt_save_freq')

    parser.add_argument('--test_num', type=int, default=10, help='The number of images generated by the test')


    parser.add_argument('--checkpoint_dir', type=str, default='checkpoint',
                        help='Directory name to save the checkpoints')
    parser.add_argument('--result_dir', type=str, default='results',
                        help='Directory name to save the generated images')
    parser.add_argument('--log_dir', type=str, default='logs',
                        help='Directory name to save training logs')
    parser.add_argument('--sample_dir', type=str, default='samples',
                        help='Directory name to save the samples on training')

    return check_args(parser.parse_args())

"""checking arguments"""
def check_args(args):
    # --checkpoint_dir
    check_folder(args.checkpoint_dir)

    # --result_dir
    check_folder(args.result_dir)

    # --result_dir
    check_folder(args.log_dir)

    # --sample_dir
    check_folder(args.sample_dir)

    # --epoch
    try:
        assert args.epoch >= 1
    except:
        print('number of epochs must be larger than or equal to one')

    # --batch_size
    try:
        assert args.batch_size >= 1
    except:
        print('batch size must be larger than or equal to one')
    return args


"""Self-Attention Based Classification Network"""
def self_attention_model(x,
                 channels, sn,
                 scope='self_attention_model'):
  # dict for return
  end_points = {}

  with tf.variable_scope(scope):
    batch_size, height, width, num_channels = x.get_shape().as_list()
    f = conv(x, channels // 8, kernel=1, stride=1, sn=sn, scope='f_conv')  # [bs, h, w, c']
    f = max_pooling(f)

    g = conv(x, channels // 8, kernel=1, stride=1, sn=sn, scope='g_conv')  # [bs, h, w, c']

    h = conv(x, channels // 2, kernel=1, stride=1, sn=sn, scope='h_conv')  # [bs, h, w, c]
    h = max_pooling(h)

    # N = h * w
    s = tf.matmul(hw_flatten(g), hw_flatten(f), transpose_b=True)  # # [bs, N, N]

    beta = tf.nn.softmax(s)  # attention map

    o = tf.matmul(beta, hw_flatten(h))  # [bs, N, C]
    gamma = tf.get_variable("gamma", [1], initializer=tf.constant_initializer(0.0))

    o = tf.reshape(o, shape=[batch_size, height, width, num_channels // 2])  # [bs, h, w, C]
    o = conv(o, channels, kernel=1, stride=1, sn=sn, scope='attn_conv')
    x = gamma * o + x

    # additional layers after selt-attention
    # conv 7 x 7
    x = batch_norm(x, is_training=True, scope='bn_1')
    x = relu(x)
    x = conv(x, 4096, kernel=7, stride=1, sn=False, scope='add_conv')  # [bs, h, w, c']

    # conv 1 x 1
    x = batch_norm(x, is_training=True, scope='bn_2')
    x = relu(x)
    x = conv(x, 4096, kernel=1, stride=1, sn=False, scope='add_conv_1x1')  # [bs, h, w, c']

    # fc with flatten
    x = batch_norm(x, is_training=True, scope='bn_3')
    x = relu(x)
    x = fully_connected(x, units=1024, sn=False, scope='fc_1')

    # softmax layer
    # num of classes (vessel ids) = 35
    x = relu(x)
    x = fully_connected(x, units=35, sn=False, scope='fc_2')
    #x = tf.nn.softmax(x)

  return x, end_points

"""main"""
def main():
    # parse arguments
    args = parse_args()
    if args is None:
      exit()

    # load maritime data
    train_features = np.load('/workspace/01_feature_extraction/Y_train.npy')
    train_labels = np.load('/workspace/01_feature_extraction/label_train.npy')
    test_features = np.load('/workspace/01_feature_extraction/Y_test.npy')
    test_labels = np.load('/workspace/01_feature_extraction/label_test.npy')

    # Assume that each row of `features` corresponds to the same row as `labels`.
    assert train_features.shape[0] == train_labels.shape[0]

    # reshape
    train_origin_shape = train_features.shape
    test_origin_shape = test_features.shape
    train_features = np.reshape(train_features, (train_origin_shape[0],
                                train_origin_shape[2], train_origin_shape[3], train_origin_shape[4]))
    test_features = np.reshape(test_features, (test_origin_shape[0],
                               test_origin_shape[2], test_origin_shape[3], test_origin_shape[4]))

    # transpose
    train_features = np.transpose(train_features, (0,2,3,1))
    test_features = np.transpose(test_features, (0,2,3,1))

    # need to make as tf record files if the size of the dataset is too big to load into memory
    train_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels))
    test_dataset = tf.data.Dataset.from_tensor_slices((test_features, test_labels))

    print (train_features.shape)
    print (train_labels.shape)
    print (test_features.shape)
    print (test_labels.shape)

    #print (train_dataset)

    """
    BATCH_SIZE = 32
    SHUFFLE_BUFFER_SIZE = 50

    train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)
    test_dataset = test_dataset.batch(BATCH_SIZE)
    """

    gpu_device = '/gpu:0'
    dataset_num = train_features.shape[0]
    batch_size = 32
    #train_dataset = train_dataset.apply(shuffle_and_repeat(dataset_num)).apply(batch_and_drop_remainder(batch_size)).apply(prefetch_to_device(gpu_device, batch_size))
    train_dataset = train_dataset.apply(shuffle_and_repeat(dataset_num)).apply(batch_and_drop_remainder(batch_size))

    # model input
    inputs = tf.placeholder(tf.float32, [batch_size, train_features.shape[1],
                            train_features.shape[2], train_features.shape[3]], name='inputs')

    # model outputs
    n_classes = 35
    labels = tf.placeholder("float", [None, n_classes])

    # model
    ch = 2048

    # spectral normalization
    sn = True
    logits, _ = self_attention_model(inputs, channels=ch, sn=sn, scope='self_attention')

    #predictions = tf.nn.softmax(x, name='prediction')

    cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels)
    loss_op = tf.reduce_mean(cross_entropy)

    # learning rate
    lr = 0.001 
    optimizer = tf.train.AdamOptimizer(learning_rate=lr)
    train_op = optimizer.minimize(loss_op)

    # Add summaries
    #tf.summary.scalar('loss', loss)

    # initialize all variables
    init = tf.global_variables_initializer()

    # open session
    config = tf.ConfigProto()
    config.allow_soft_placement = True
    config.gpu_options.per_process_gpu_memory_fraction = 0.5
    config.gpu_options.allow_growth = True


    with tf.Session(config=config) as sess:

        # initialize
        sess.run(init)

        if args.phase == 'train' :
            avg_cost = 0.
            total_batch = int(dataset_num/batch_size)
            iter = train_dataset.make_one_shot_iterator()
            #print (total_batch)
            for i in range(total_batch):
                batch_x, batch_y = iter.get_next()
                print (sess.run(batch_y.eval()))
                # Run optimization op (backprop) and cost op (to get loss value)
                #_, c = sess.run([train_op, loss_op], feed_dict={inputs: batch_x.eval(), labels: batch_y.eval()})

                # Compute average loss
                avg_cost += c / total_batch
                #print("Epoch:", '%04d' % (epoch+1), "cost={:.9f}".format(avg_cost))
                print("cost={:.9f}".format(avg_cost))

            print(" [*] Training finished!")

        if args.phase == 'test' :
            #gan.test()
            print(" [*] Test finished!")

if __name__ == '__main__':
    main()
