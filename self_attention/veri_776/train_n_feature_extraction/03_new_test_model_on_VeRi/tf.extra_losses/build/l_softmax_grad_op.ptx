//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-22781540
// Cuda compilation tools, release 9.0, V9.0.176
// Based on LLVM 3.4svn
//

.version 6.0
.target sm_61
.address_size 64

	// .globl	_Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA_
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd
(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
;
.func  (.param .b64 func_retval0) __internal_accurate_pow
(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
;
.const .align 8 .b8 __cudart_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};
.const .align 8 .b8 __cudart_sin_cos_coeffs[128] = {186, 94, 120, 249, 101, 219, 229, 61, 70, 210, 176, 44, 241, 229, 90, 190, 146, 227, 172, 105, 227, 29, 199, 62, 161, 98, 219, 25, 160, 1, 42, 191, 24, 8, 17, 17, 17, 17, 129, 63, 84, 85, 85, 85, 85, 85, 197, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 129, 253, 32, 131, 255, 168, 189, 40, 133, 239, 193, 167, 238, 33, 62, 217, 230, 6, 142, 79, 126, 146, 190, 233, 188, 221, 25, 160, 1, 250, 62, 71, 93, 193, 22, 108, 193, 86, 191, 81, 85, 85, 85, 85, 85, 165, 63, 0, 0, 0, 0, 0, 0, 224, 191, 0, 0, 0, 0, 0, 0, 240, 63};

.visible .entry _Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA_(
	.param .align 4 .b8 _Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_0[12],
	.param .u64 _Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_1,
	.param .u64 _Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_2,
	.param .u64 _Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_3,
	.param .u64 _Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_4,
	.param .u64 _Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_5,
	.param .u32 _Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_6,
	.param .u32 _Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_7,
	.param .u32 _Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_8,
	.param .u32 _Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_9,
	.param .u8 _Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_10,
	.param .u64 _Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_11,
	.param .u64 _Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_12,
	.param .u64 _Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_13,
	.param .u64 _Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_14,
	.param .u64 _Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_15,
	.param .u64 _Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_16
)
{
	.local .align 4 .b8 	__local_depot0[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<372>;
	.reg .b16 	%rs<6>;
	.reg .f32 	%f<277>;
	.reg .b32 	%r<826>;
	.reg .f64 	%fd<693>;
	.reg .b64 	%rd<293>;


	mov.u64 	%rd292, __local_depot0;
	cvta.local.u64 	%SP, %rd292;
	ld.param.u32 	%r221, [_Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_0];
	ld.param.u64 	%rd63, [_Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_2];
	ld.param.u64 	%rd64, [_Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_3];
	ld.param.u64 	%rd65, [_Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_4];
	ld.param.u32 	%r224, [_Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_6];
	ld.param.u32 	%r225, [_Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_7];
	ld.param.u32 	%r226, [_Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_8];
	ld.param.u32 	%r227, [_Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_9];
	ld.param.u64 	%rd67, [_Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_11];
	ld.param.u64 	%rd68, [_Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_12];
	ld.param.u64 	%rd69, [_Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_13];
	ld.param.u64 	%rd70, [_Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_14];
	ld.param.u64 	%rd71, [_Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_15];
	ld.param.u64 	%rd72, [_Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_16];
	ld.param.s8 	%rs1, [_Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_10];
	cvta.to.global.u64 	%rd1, %rd71;
	cvta.to.global.u64 	%rd2, %rd72;
	cvta.to.global.u64 	%rd3, %rd69;
	cvta.to.global.u64 	%rd4, %rd70;
	cvta.to.global.u64 	%rd5, %rd68;
	cvta.to.global.u64 	%rd6, %rd67;
	add.u64 	%rd73, %SP, 0;
	cvta.to.local.u64 	%rd7, %rd73;
	setp.lt.s32	%p15, %r224, 1;
	@%p15 bra 	BB0_29;

	mov.f64 	%fd258, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd258;
	}
	bfe.u32 	%r229, %r1, 20, 11;
	add.s32 	%r230, %r229, -1012;
	mov.u64 	%rd74, 4602678819172646912;
	shl.b64 	%rd8, %rd74, %r230;
	and.b32  	%r2, %r1, 2147483647;
	shr.s32 	%r231, %r1, 31;
	and.b32  	%r232, %r231, -2146435072;
	add.s32 	%r3, %r232, 2146435072;
	or.b32  	%r4, %r3, -2147483648;
	and.b32  	%r5, %r225, 3;
	mov.u32 	%r752, 0;
	bra.uni 	BB0_2;

BB0_23:
	and.b32  	%r250, %r15, 2147483647;
	setp.ne.s32	%p33, %r250, 2146435072;
	@%p33 bra 	BB0_24;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r251, %temp}, %fd1;
	}
	setp.ne.s32	%p34, %r251, 0;
	mov.f64 	%fd623, %fd622;
	@%p34 bra 	BB0_28;

	selp.b32	%r252, %r4, %r3, %p1;
	mov.u32 	%r253, 0;
	mov.b64 	%fd623, {%r253, %r252};
	bra.uni 	BB0_28;

BB0_24:
	mov.f64 	%fd623, %fd622;
	bra.uni 	BB0_28;

BB0_2:
	mul.lo.s32 	%r7, %r752, %r225;
	mov.f32 	%f264, 0f00000000;
	setp.lt.s32	%p16, %r225, 1;
	@%p16 bra 	BB0_12;

	mov.f32 	%f264, 0f00000000;
	mov.u32 	%r755, 0;
	setp.eq.s32	%p17, %r5, 0;
	@%p17 bra 	BB0_9;

	setp.eq.s32	%p18, %r5, 1;
	@%p18 bra 	BB0_8;

	setp.eq.s32	%p19, %r5, 2;
	@%p19 bra 	BB0_7;

	mul.wide.s32 	%rd77, %r7, 4;
	add.s64 	%rd76, %rd63, %rd77;
	// inline asm
	ld.global.nc.f32 %f46, [%rd76];
	// inline asm
	// inline asm
	ld.global.nc.f32 %f47, [%rd76];
	// inline asm
	fma.rn.f32 	%f264, %f46, %f47, 0f00000000;
	mov.u32 	%r755, 1;

BB0_7:
	add.s32 	%r237, %r755, %r7;
	mul.wide.s32 	%rd80, %r237, 4;
	add.s64 	%rd79, %rd63, %rd80;
	// inline asm
	ld.global.nc.f32 %f48, [%rd79];
	// inline asm
	// inline asm
	ld.global.nc.f32 %f49, [%rd79];
	// inline asm
	fma.rn.f32 	%f264, %f48, %f49, %f264;
	add.s32 	%r755, %r755, 1;

BB0_8:
	add.s32 	%r238, %r755, %r7;
	mul.wide.s32 	%rd83, %r238, 4;
	add.s64 	%rd82, %rd63, %rd83;
	// inline asm
	ld.global.nc.f32 %f50, [%rd82];
	// inline asm
	// inline asm
	ld.global.nc.f32 %f51, [%rd82];
	// inline asm
	fma.rn.f32 	%f264, %f50, %f51, %f264;
	add.s32 	%r755, %r755, 1;

BB0_9:
	setp.lt.u32	%p20, %r225, 4;
	@%p20 bra 	BB0_12;

	mad.lo.s32 	%r239, %r225, %r752, %r755;
	mul.wide.s32 	%rd84, %r239, 4;
	add.s64 	%rd282, %rd63, %rd84;

BB0_11:
	// inline asm
	ld.global.nc.f32 %f52, [%rd282];
	// inline asm
	// inline asm
	ld.global.nc.f32 %f53, [%rd282];
	// inline asm
	fma.rn.f32 	%f60, %f52, %f53, %f264;
	add.s64 	%rd88, %rd282, 4;
	// inline asm
	ld.global.nc.f32 %f54, [%rd88];
	// inline asm
	// inline asm
	ld.global.nc.f32 %f55, [%rd88];
	// inline asm
	fma.rn.f32 	%f61, %f54, %f55, %f60;
	add.s64 	%rd90, %rd282, 8;
	// inline asm
	ld.global.nc.f32 %f56, [%rd90];
	// inline asm
	// inline asm
	ld.global.nc.f32 %f57, [%rd90];
	// inline asm
	fma.rn.f32 	%f62, %f56, %f57, %f61;
	add.s64 	%rd92, %rd282, 12;
	// inline asm
	ld.global.nc.f32 %f58, [%rd92];
	// inline asm
	// inline asm
	ld.global.nc.f32 %f59, [%rd92];
	// inline asm
	fma.rn.f32 	%f264, %f58, %f59, %f62;
	add.s64 	%rd282, %rd282, 16;
	add.s32 	%r755, %r755, 4;
	setp.lt.s32	%p21, %r755, %r225;
	@%p21 bra 	BB0_11;

BB0_12:
	cvt.f64.f32	%fd1, %f264;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd1;
	}
	abs.f64 	%fd2, %fd1;
	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd2;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd258;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd622, [retval0+0];
	
	//{
	}// Callseq End 0
	setp.lt.s32	%p22, %r15, 0;
	setp.eq.s64	%p23, %rd8, -9223372036854775808;
	and.pred  	%p1, %p22, %p23;
	@!%p1 bra 	BB0_14;
	bra.uni 	BB0_13;

BB0_13:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r240}, %fd622;
	}
	xor.b32  	%r241, %r240, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r242, %temp}, %fd622;
	}
	mov.b64 	%fd622, {%r242, %r241};

BB0_14:
	setp.eq.f32	%p24, %f264, 0f00000000;
	@%p24 bra 	BB0_17;
	bra.uni 	BB0_15;

BB0_17:
	setp.lt.s32	%p27, %r1, 0;
	selp.b32	%r243, %r15, 0, %p23;
	or.b32  	%r244, %r243, 2146435072;
	selp.b32	%r245, %r244, %r243, %p27;
	mov.u32 	%r246, 0;
	mov.b64 	%fd622, {%r246, %r245};
	bra.uni 	BB0_18;

BB0_15:
	setp.gt.s32	%p25, %r15, -1;
	@%p25 bra 	BB0_18;

	cvt.rzi.f64.f64	%fd261, %fd258;
	setp.neu.f64	%p26, %fd261, 0d3FE0000000000000;
	selp.f64	%fd622, 0dFFF8000000000000, %fd622, %p26;

BB0_18:
	add.f64 	%fd623, %fd1, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r247}, %fd623;
	}
	and.b32  	%r248, %r247, 2146435072;
	setp.ne.s32	%p29, %r248, 2146435072;
	@%p29 bra 	BB0_19;

	setp.gtu.f64	%p30, %fd2, 0d7FF0000000000000;
	@%p30 bra 	BB0_28;

	setp.ne.s32	%p31, %r2, 2146435072;
	@%p31 bra 	BB0_23;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r249, %temp}, %fd258;
	}
	setp.eq.s32	%p32, %r249, 0;
	@%p32 bra 	BB0_27;
	bra.uni 	BB0_23;

BB0_27:
	setp.lt.s32	%p35, %r1, 0;
	setp.gt.f64	%p36, %fd2, 0d3FF0000000000000;
	selp.b32	%r254, 2146435072, 0, %p36;
	xor.b32  	%r255, %r254, 2146435072;
	selp.b32	%r256, %r255, %r254, %p35;
	setp.eq.f32	%p37, %f264, 0fBF800000;
	selp.b32	%r257, 1072693248, %r256, %p37;
	mov.u32 	%r258, 0;
	mov.b64 	%fd623, {%r258, %r257};
	bra.uni 	BB0_28;

BB0_19:
	mov.f64 	%fd623, %fd622;

BB0_28:
	cvt.rn.f32.f64	%f63, %fd623;
	setp.eq.f32	%p38, %f264, 0f3F800000;
	selp.f32	%f64, 0f3F800000, %f63, %p38;
	mul.wide.s32 	%rd93, %r752, 4;
	add.s64 	%rd94, %rd6, %rd93;
	st.global.f32 	[%rd94], %f64;
	add.s32 	%r752, %r752, 1;
	setp.lt.s32	%p39, %r752, %r224;
	@%p39 bra 	BB0_2;

BB0_29:
	setp.lt.s32	%p40, %r226, 1;
	@%p40 bra 	BB0_60;

	and.b32  	%r17, %r225, 3;
	mov.u32 	%r757, 0;
	bra.uni 	BB0_31;

BB0_53:
	and.b32  	%r280, %r27, 2147483647;
	setp.ne.s32	%p59, %r280, 2146435072;
	@%p59 bra 	BB0_54;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r281, %temp}, %fd13;
	}
	setp.ne.s32	%p60, %r281, 0;
	mov.f64 	%fd626, %fd625;
	@%p60 bra 	BB0_58;

	shr.s32 	%r282, %r28, 31;
	and.b32  	%r283, %r282, -2146435072;
	add.s32 	%r284, %r283, 2146435072;
	or.b32  	%r285, %r284, -2147483648;
	selp.b32	%r286, %r285, %r284, %p2;
	mov.u32 	%r287, 0;
	mov.b64 	%fd626, {%r287, %r286};
	bra.uni 	BB0_58;

BB0_54:
	mov.f64 	%fd626, %fd625;
	bra.uni 	BB0_58;

BB0_31:
	mul.lo.s32 	%r19, %r757, %r225;
	mov.f32 	%f269, 0f00000000;
	setp.lt.s32	%p41, %r225, 1;
	@%p41 bra 	BB0_41;

	mov.f32 	%f269, 0f00000000;
	mov.u32 	%r760, 0;
	setp.eq.s32	%p42, %r17, 0;
	@%p42 bra 	BB0_38;

	setp.eq.s32	%p43, %r17, 1;
	@%p43 bra 	BB0_37;

	setp.eq.s32	%p44, %r17, 2;
	@%p44 bra 	BB0_36;

	mul.wide.s32 	%rd97, %r19, 4;
	add.s64 	%rd96, %rd64, %rd97;
	// inline asm
	ld.global.nc.f32 %f69, [%rd96];
	// inline asm
	// inline asm
	ld.global.nc.f32 %f70, [%rd96];
	// inline asm
	fma.rn.f32 	%f269, %f69, %f70, 0f00000000;
	mov.u32 	%r760, 1;

BB0_36:
	add.s32 	%r264, %r760, %r19;
	mul.wide.s32 	%rd100, %r264, 4;
	add.s64 	%rd99, %rd64, %rd100;
	// inline asm
	ld.global.nc.f32 %f71, [%rd99];
	// inline asm
	// inline asm
	ld.global.nc.f32 %f72, [%rd99];
	// inline asm
	fma.rn.f32 	%f269, %f71, %f72, %f269;
	add.s32 	%r760, %r760, 1;

BB0_37:
	add.s32 	%r265, %r760, %r19;
	mul.wide.s32 	%rd103, %r265, 4;
	add.s64 	%rd102, %rd64, %rd103;
	// inline asm
	ld.global.nc.f32 %f73, [%rd102];
	// inline asm
	// inline asm
	ld.global.nc.f32 %f74, [%rd102];
	// inline asm
	fma.rn.f32 	%f269, %f73, %f74, %f269;
	add.s32 	%r760, %r760, 1;

BB0_38:
	setp.lt.u32	%p45, %r225, 4;
	@%p45 bra 	BB0_41;

	mad.lo.s32 	%r266, %r225, %r757, %r760;
	mul.wide.s32 	%rd104, %r266, 4;
	add.s64 	%rd283, %rd64, %rd104;

BB0_40:
	// inline asm
	ld.global.nc.f32 %f75, [%rd283];
	// inline asm
	// inline asm
	ld.global.nc.f32 %f76, [%rd283];
	// inline asm
	fma.rn.f32 	%f83, %f75, %f76, %f269;
	add.s64 	%rd108, %rd283, 4;
	// inline asm
	ld.global.nc.f32 %f77, [%rd108];
	// inline asm
	// inline asm
	ld.global.nc.f32 %f78, [%rd108];
	// inline asm
	fma.rn.f32 	%f84, %f77, %f78, %f83;
	add.s64 	%rd110, %rd283, 8;
	// inline asm
	ld.global.nc.f32 %f79, [%rd110];
	// inline asm
	// inline asm
	ld.global.nc.f32 %f80, [%rd110];
	// inline asm
	fma.rn.f32 	%f85, %f79, %f80, %f84;
	add.s64 	%rd112, %rd283, 12;
	// inline asm
	ld.global.nc.f32 %f81, [%rd112];
	// inline asm
	// inline asm
	ld.global.nc.f32 %f82, [%rd112];
	// inline asm
	fma.rn.f32 	%f269, %f81, %f82, %f85;
	add.s64 	%rd283, %rd283, 16;
	add.s32 	%r760, %r760, 4;
	setp.lt.s32	%p46, %r760, %r225;
	@%p46 bra 	BB0_40;

BB0_41:
	and.b16  	%rs2, %rs1, 255;
	mov.f64 	%fd627, 0d3FF0000000000000;
	setp.ne.s16	%p47, %rs2, 0;
	@%p47 bra 	BB0_59;

	cvt.f64.f32	%fd13, %f269;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r27}, %fd13;
	}
	mov.f64 	%fd264, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd264;
	}
	bfe.u32 	%r267, %r28, 20, 11;
	add.s32 	%r268, %r267, -1012;
	mov.u64 	%rd113, 4602678819172646912;
	shl.b64 	%rd15, %rd113, %r268;
	setp.eq.s64	%p48, %rd15, -9223372036854775808;
	abs.f64 	%fd14, %fd13;
	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd14;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd264;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd625, [retval0+0];
	
	//{
	}// Callseq End 1
	setp.lt.s32	%p49, %r27, 0;
	and.pred  	%p2, %p49, %p48;
	@!%p2 bra 	BB0_44;
	bra.uni 	BB0_43;

BB0_43:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r269}, %fd625;
	}
	xor.b32  	%r270, %r269, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r271, %temp}, %fd625;
	}
	mov.b64 	%fd625, {%r271, %r270};

BB0_44:
	setp.eq.f32	%p50, %f269, 0f00000000;
	@%p50 bra 	BB0_47;
	bra.uni 	BB0_45;

BB0_47:
	selp.b32	%r272, %r27, 0, %p48;
	or.b32  	%r273, %r272, 2146435072;
	setp.lt.s32	%p54, %r28, 0;
	selp.b32	%r274, %r273, %r272, %p54;
	mov.u32 	%r275, 0;
	mov.b64 	%fd625, {%r275, %r274};
	bra.uni 	BB0_48;

BB0_45:
	setp.gt.s32	%p51, %r27, -1;
	@%p51 bra 	BB0_48;

	cvt.rzi.f64.f64	%fd266, %fd264;
	setp.neu.f64	%p52, %fd266, 0d3FE0000000000000;
	selp.f64	%fd625, 0dFFF8000000000000, %fd625, %p52;

BB0_48:
	add.f64 	%fd626, %fd13, 0d3FE0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r276}, %fd626;
	}
	and.b32  	%r277, %r276, 2146435072;
	setp.ne.s32	%p55, %r277, 2146435072;
	@%p55 bra 	BB0_49;

	setp.gtu.f64	%p56, %fd14, 0d7FF0000000000000;
	@%p56 bra 	BB0_58;

	and.b32  	%r278, %r28, 2147483647;
	setp.ne.s32	%p57, %r278, 2146435072;
	@%p57 bra 	BB0_53;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r279, %temp}, %fd264;
	}
	setp.eq.s32	%p58, %r279, 0;
	@%p58 bra 	BB0_57;
	bra.uni 	BB0_53;

BB0_57:
	setp.gt.f64	%p61, %fd14, 0d3FF0000000000000;
	selp.b32	%r288, 2146435072, 0, %p61;
	xor.b32  	%r289, %r288, 2146435072;
	setp.lt.s32	%p62, %r28, 0;
	selp.b32	%r290, %r289, %r288, %p62;
	setp.eq.f32	%p63, %f269, 0fBF800000;
	selp.b32	%r291, 1072693248, %r290, %p63;
	mov.u32 	%r292, 0;
	mov.b64 	%fd626, {%r292, %r291};
	bra.uni 	BB0_58;

BB0_49:
	mov.f64 	%fd626, %fd625;

BB0_58:
	setp.eq.f32	%p64, %f269, 0f3F800000;
	selp.f64	%fd627, 0d3FF0000000000000, %fd626, %p64;

BB0_59:
	mul.wide.s32 	%rd114, %r757, 4;
	add.s64 	%rd115, %rd5, %rd114;
	cvt.rn.f32.f64	%f86, %fd627;
	st.global.f32 	[%rd115], %f86;
	add.s32 	%r757, %r757, 1;
	setp.lt.s32	%p65, %r757, %r226;
	@%p65 bra 	BB0_31;

BB0_60:
	setp.lt.s32	%p66, %r227, 1;
	@%p66 bra 	BB0_133;

	cvt.rn.f64.s32	%fd27, %r227;
	and.b32  	%r296, %r227, 3;
	mov.u32 	%r763, 0;
	setp.eq.s32	%p67, %r296, 0;
	@%p67 bra 	BB0_94;

	setp.eq.s32	%p68, %r296, 1;
	@%p68 bra 	BB0_84;

	setp.eq.s32	%p69, %r296, 2;
	@%p69 bra 	BB0_74;

	mov.f64 	%fd268, 0d0000000000000000;
	div.rn.f64 	%fd628, %fd268, %fd27;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r297}, %fd628;
	}
	and.b32  	%r298, %r297, 2147483647;
	setp.ne.s32	%p70, %r298, 2146435072;
	@%p70 bra 	BB0_67;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r299, %temp}, %fd628;
	}
	setp.ne.s32	%p71, %r299, 0;
	@%p71 bra 	BB0_67;

	mul.rn.f64 	%fd628, %fd628, %fd268;

BB0_67:
	mul.f64 	%fd270, %fd628, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r762, %fd270;
	st.local.u32 	[%rd7], %r762;
	cvt.rn.f64.s32	%fd271, %r762;
	neg.f64 	%fd272, %fd271;
	mov.f64 	%fd273, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd274, %fd272, %fd273, %fd628;
	mov.f64 	%fd275, 0d3C91A62633145C00;
	fma.rn.f64 	%fd276, %fd272, %fd275, %fd274;
	mov.f64 	%fd277, 0d397B839A252049C0;
	fma.rn.f64 	%fd629, %fd272, %fd277, %fd276;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r300}, %fd628;
	}
	and.b32  	%r301, %r300, 2145386496;
	setp.lt.u32	%p72, %r301, 1105199104;
	@%p72 bra 	BB0_69;

	// Callseq Start 2
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd628;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd73;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd629, [retval0+0];
	
	//{
	}// Callseq End 2
	ld.local.u32 	%r762, [%rd7];

BB0_69:
	add.s32 	%r33, %r762, 1;
	and.b32  	%r302, %r33, 1;
	shl.b32 	%r303, %r302, 3;
	setp.eq.s32	%p73, %r302, 0;
	selp.f64	%fd278, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p73;
	add.s32 	%r304, %r303, 1;
	mul.wide.s32 	%rd117, %r304, 8;
	mov.u64 	%rd118, __cudart_sin_cos_coeffs;
	add.s64 	%rd119, %rd118, %rd117;
	ld.const.f64 	%fd279, [%rd119];
	mul.rn.f64 	%fd34, %fd629, %fd629;
	fma.rn.f64 	%fd280, %fd278, %fd34, %fd279;
	ld.const.f64 	%fd281, [%rd119+8];
	fma.rn.f64 	%fd282, %fd280, %fd34, %fd281;
	ld.const.f64 	%fd283, [%rd119+16];
	fma.rn.f64 	%fd284, %fd282, %fd34, %fd283;
	ld.const.f64 	%fd285, [%rd119+24];
	fma.rn.f64 	%fd286, %fd284, %fd34, %fd285;
	ld.const.f64 	%fd287, [%rd119+32];
	fma.rn.f64 	%fd288, %fd286, %fd34, %fd287;
	ld.const.f64 	%fd289, [%rd119+40];
	fma.rn.f64 	%fd35, %fd288, %fd34, %fd289;
	fma.rn.f64 	%fd630, %fd35, %fd629, %fd629;
	@%p73 bra 	BB0_71;

	mov.f64 	%fd290, 0d3FF0000000000000;
	fma.rn.f64 	%fd630, %fd35, %fd34, %fd290;

BB0_71:
	and.b32  	%r305, %r33, 2;
	setp.eq.s32	%p74, %r305, 0;
	@%p74 bra 	BB0_73;

	mov.f64 	%fd292, 0dBFF0000000000000;
	fma.rn.f64 	%fd630, %fd630, %fd292, %fd268;

BB0_73:
	cvt.rn.f32.f64	%f87, %fd630;
	st.global.f32 	[%rd4], %f87;
	mov.u32 	%r763, 1;

BB0_74:
	cvt.rn.f64.s32	%fd293, %r763;
	mul.f64 	%fd294, %fd293, 0d400921FB54442D18;
	div.rn.f64 	%fd632, %fd294, %fd27;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r307}, %fd632;
	}
	and.b32  	%r308, %r307, 2147483647;
	setp.ne.s32	%p75, %r308, 2146435072;
	@%p75 bra 	BB0_77;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r309, %temp}, %fd632;
	}
	setp.ne.s32	%p76, %r309, 0;
	@%p76 bra 	BB0_77;

	mov.f64 	%fd295, 0d0000000000000000;
	mul.rn.f64 	%fd632, %fd632, %fd295;

BB0_77:
	mul.f64 	%fd296, %fd632, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r764, %fd296;
	st.local.u32 	[%rd7], %r764;
	cvt.rn.f64.s32	%fd297, %r764;
	neg.f64 	%fd298, %fd297;
	mov.f64 	%fd299, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd300, %fd298, %fd299, %fd632;
	mov.f64 	%fd301, 0d3C91A62633145C00;
	fma.rn.f64 	%fd302, %fd298, %fd301, %fd300;
	mov.f64 	%fd303, 0d397B839A252049C0;
	fma.rn.f64 	%fd633, %fd298, %fd303, %fd302;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r310}, %fd632;
	}
	and.b32  	%r311, %r310, 2145386496;
	setp.lt.u32	%p77, %r311, 1105199104;
	@%p77 bra 	BB0_79;

	// Callseq Start 3
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd632;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd73;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd633, [retval0+0];
	
	//{
	}// Callseq End 3
	ld.local.u32 	%r764, [%rd7];

BB0_79:
	add.s32 	%r38, %r764, 1;
	and.b32  	%r312, %r38, 1;
	shl.b32 	%r313, %r312, 3;
	setp.eq.s32	%p78, %r312, 0;
	selp.f64	%fd304, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p78;
	add.s32 	%r314, %r313, 1;
	mul.wide.s32 	%rd121, %r314, 8;
	mov.u64 	%rd122, __cudart_sin_cos_coeffs;
	add.s64 	%rd123, %rd122, %rd121;
	ld.const.f64 	%fd305, [%rd123];
	mul.rn.f64 	%fd47, %fd633, %fd633;
	fma.rn.f64 	%fd306, %fd304, %fd47, %fd305;
	ld.const.f64 	%fd307, [%rd123+8];
	fma.rn.f64 	%fd308, %fd306, %fd47, %fd307;
	ld.const.f64 	%fd309, [%rd123+16];
	fma.rn.f64 	%fd310, %fd308, %fd47, %fd309;
	ld.const.f64 	%fd311, [%rd123+24];
	fma.rn.f64 	%fd312, %fd310, %fd47, %fd311;
	ld.const.f64 	%fd313, [%rd123+32];
	fma.rn.f64 	%fd314, %fd312, %fd47, %fd313;
	ld.const.f64 	%fd315, [%rd123+40];
	fma.rn.f64 	%fd48, %fd314, %fd47, %fd315;
	fma.rn.f64 	%fd634, %fd48, %fd633, %fd633;
	@%p78 bra 	BB0_81;

	mov.f64 	%fd316, 0d3FF0000000000000;
	fma.rn.f64 	%fd634, %fd48, %fd47, %fd316;

BB0_81:
	and.b32  	%r315, %r38, 2;
	setp.eq.s32	%p79, %r315, 0;
	@%p79 bra 	BB0_83;

	mov.f64 	%fd317, 0d0000000000000000;
	mov.f64 	%fd318, 0dBFF0000000000000;
	fma.rn.f64 	%fd634, %fd634, %fd318, %fd317;

BB0_83:
	mul.wide.u32 	%rd124, %r763, 4;
	add.s64 	%rd125, %rd4, %rd124;
	cvt.rn.f32.f64	%f88, %fd634;
	st.global.f32 	[%rd125], %f88;
	add.s32 	%r763, %r763, 1;

BB0_84:
	cvt.rn.f64.s32	%fd319, %r763;
	mul.f64 	%fd320, %fd319, 0d400921FB54442D18;
	div.rn.f64 	%fd636, %fd320, %fd27;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r316}, %fd636;
	}
	and.b32  	%r317, %r316, 2147483647;
	setp.ne.s32	%p80, %r317, 2146435072;
	@%p80 bra 	BB0_87;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r318, %temp}, %fd636;
	}
	setp.ne.s32	%p81, %r318, 0;
	@%p81 bra 	BB0_87;

	mov.f64 	%fd321, 0d0000000000000000;
	mul.rn.f64 	%fd636, %fd636, %fd321;

BB0_87:
	mul.f64 	%fd322, %fd636, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r766, %fd322;
	st.local.u32 	[%rd7], %r766;
	cvt.rn.f64.s32	%fd323, %r766;
	neg.f64 	%fd324, %fd323;
	mov.f64 	%fd325, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd326, %fd324, %fd325, %fd636;
	mov.f64 	%fd327, 0d3C91A62633145C00;
	fma.rn.f64 	%fd328, %fd324, %fd327, %fd326;
	mov.f64 	%fd329, 0d397B839A252049C0;
	fma.rn.f64 	%fd637, %fd324, %fd329, %fd328;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r319}, %fd636;
	}
	and.b32  	%r320, %r319, 2145386496;
	setp.lt.u32	%p82, %r320, 1105199104;
	@%p82 bra 	BB0_89;

	// Callseq Start 4
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd636;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd73;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd637, [retval0+0];
	
	//{
	}// Callseq End 4
	ld.local.u32 	%r766, [%rd7];

BB0_89:
	add.s32 	%r44, %r766, 1;
	and.b32  	%r321, %r44, 1;
	shl.b32 	%r322, %r321, 3;
	setp.eq.s32	%p83, %r321, 0;
	selp.f64	%fd330, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p83;
	add.s32 	%r323, %r322, 1;
	mul.wide.s32 	%rd127, %r323, 8;
	mov.u64 	%rd128, __cudart_sin_cos_coeffs;
	add.s64 	%rd129, %rd128, %rd127;
	ld.const.f64 	%fd331, [%rd129];
	mul.rn.f64 	%fd60, %fd637, %fd637;
	fma.rn.f64 	%fd332, %fd330, %fd60, %fd331;
	ld.const.f64 	%fd333, [%rd129+8];
	fma.rn.f64 	%fd334, %fd332, %fd60, %fd333;
	ld.const.f64 	%fd335, [%rd129+16];
	fma.rn.f64 	%fd336, %fd334, %fd60, %fd335;
	ld.const.f64 	%fd337, [%rd129+24];
	fma.rn.f64 	%fd338, %fd336, %fd60, %fd337;
	ld.const.f64 	%fd339, [%rd129+32];
	fma.rn.f64 	%fd340, %fd338, %fd60, %fd339;
	ld.const.f64 	%fd341, [%rd129+40];
	fma.rn.f64 	%fd61, %fd340, %fd60, %fd341;
	fma.rn.f64 	%fd638, %fd61, %fd637, %fd637;
	@%p83 bra 	BB0_91;

	mov.f64 	%fd342, 0d3FF0000000000000;
	fma.rn.f64 	%fd638, %fd61, %fd60, %fd342;

BB0_91:
	and.b32  	%r324, %r44, 2;
	setp.eq.s32	%p84, %r324, 0;
	@%p84 bra 	BB0_93;

	mov.f64 	%fd343, 0d0000000000000000;
	mov.f64 	%fd344, 0dBFF0000000000000;
	fma.rn.f64 	%fd638, %fd638, %fd344, %fd343;

BB0_93:
	mul.wide.s32 	%rd130, %r763, 4;
	add.s64 	%rd131, %rd4, %rd130;
	cvt.rn.f32.f64	%f89, %fd638;
	st.global.f32 	[%rd131], %f89;
	add.s32 	%r763, %r763, 1;

BB0_94:
	setp.lt.u32	%p85, %r227, 4;
	@%p85 bra 	BB0_133;

	mul.wide.s32 	%rd132, %r763, 4;
	add.s64 	%rd284, %rd4, %rd132;

BB0_96:
	cvt.rn.f64.s32	%fd345, %r763;
	mul.f64 	%fd346, %fd345, 0d400921FB54442D18;
	div.rn.f64 	%fd640, %fd346, %fd27;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r325}, %fd640;
	}
	and.b32  	%r326, %r325, 2147483647;
	setp.ne.s32	%p86, %r326, 2146435072;
	@%p86 bra 	BB0_99;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r327, %temp}, %fd640;
	}
	setp.ne.s32	%p87, %r327, 0;
	@%p87 bra 	BB0_99;

	mov.f64 	%fd347, 0d0000000000000000;
	mul.rn.f64 	%fd640, %fd640, %fd347;

BB0_99:
	mul.f64 	%fd348, %fd640, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r769, %fd348;
	st.local.u32 	[%rd7], %r769;
	cvt.rn.f64.s32	%fd349, %r769;
	neg.f64 	%fd350, %fd349;
	mov.f64 	%fd351, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd352, %fd350, %fd351, %fd640;
	mov.f64 	%fd353, 0d3C91A62633145C00;
	fma.rn.f64 	%fd354, %fd350, %fd353, %fd352;
	mov.f64 	%fd355, 0d397B839A252049C0;
	fma.rn.f64 	%fd641, %fd350, %fd355, %fd354;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r328}, %fd640;
	}
	and.b32  	%r329, %r328, 2145386496;
	setp.lt.u32	%p88, %r329, 1105199104;
	@%p88 bra 	BB0_101;

	// Callseq Start 5
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd640;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd73;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd641, [retval0+0];
	
	//{
	}// Callseq End 5
	ld.local.u32 	%r769, [%rd7];

BB0_101:
	add.s32 	%r51, %r769, 1;
	and.b32  	%r330, %r51, 1;
	shl.b32 	%r331, %r330, 3;
	setp.eq.s32	%p89, %r330, 0;
	selp.f64	%fd356, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p89;
	add.s32 	%r332, %r331, 1;
	mul.wide.s32 	%rd134, %r332, 8;
	mov.u64 	%rd135, __cudart_sin_cos_coeffs;
	add.s64 	%rd136, %rd135, %rd134;
	ld.const.f64 	%fd357, [%rd136];
	mul.rn.f64 	%fd73, %fd641, %fd641;
	fma.rn.f64 	%fd358, %fd356, %fd73, %fd357;
	ld.const.f64 	%fd359, [%rd136+8];
	fma.rn.f64 	%fd360, %fd358, %fd73, %fd359;
	ld.const.f64 	%fd361, [%rd136+16];
	fma.rn.f64 	%fd362, %fd360, %fd73, %fd361;
	ld.const.f64 	%fd363, [%rd136+24];
	fma.rn.f64 	%fd364, %fd362, %fd73, %fd363;
	ld.const.f64 	%fd365, [%rd136+32];
	fma.rn.f64 	%fd366, %fd364, %fd73, %fd365;
	ld.const.f64 	%fd367, [%rd136+40];
	fma.rn.f64 	%fd74, %fd366, %fd73, %fd367;
	fma.rn.f64 	%fd642, %fd74, %fd641, %fd641;
	@%p89 bra 	BB0_103;

	mov.f64 	%fd368, 0d3FF0000000000000;
	fma.rn.f64 	%fd642, %fd74, %fd73, %fd368;

BB0_103:
	and.b32  	%r333, %r51, 2;
	setp.eq.s32	%p90, %r333, 0;
	@%p90 bra 	BB0_105;

	mov.f64 	%fd369, 0d0000000000000000;
	mov.f64 	%fd370, 0dBFF0000000000000;
	fma.rn.f64 	%fd642, %fd642, %fd370, %fd369;

BB0_105:
	cvt.rn.f32.f64	%f90, %fd642;
	st.global.f32 	[%rd284], %f90;
	add.s32 	%r52, %r763, 1;
	cvt.rn.f64.s32	%fd371, %r52;
	mul.f64 	%fd372, %fd371, 0d400921FB54442D18;
	div.rn.f64 	%fd644, %fd372, %fd27;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r334}, %fd644;
	}
	and.b32  	%r335, %r334, 2147483647;
	setp.ne.s32	%p91, %r335, 2146435072;
	@%p91 bra 	BB0_108;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r336, %temp}, %fd644;
	}
	setp.ne.s32	%p92, %r336, 0;
	@%p92 bra 	BB0_108;

	mov.f64 	%fd373, 0d0000000000000000;
	mul.rn.f64 	%fd644, %fd644, %fd373;

BB0_108:
	mul.f64 	%fd374, %fd644, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r770, %fd374;
	st.local.u32 	[%rd7], %r770;
	cvt.rn.f64.s32	%fd375, %r770;
	neg.f64 	%fd376, %fd375;
	fma.rn.f64 	%fd378, %fd376, %fd351, %fd644;
	fma.rn.f64 	%fd380, %fd376, %fd353, %fd378;
	fma.rn.f64 	%fd645, %fd376, %fd355, %fd380;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r337}, %fd644;
	}
	and.b32  	%r338, %r337, 2145386496;
	setp.lt.u32	%p93, %r338, 1105199104;
	@%p93 bra 	BB0_110;

	// Callseq Start 6
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd644;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd73;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd645, [retval0+0];
	
	//{
	}// Callseq End 6
	ld.local.u32 	%r770, [%rd7];

BB0_110:
	add.s32 	%r56, %r770, 1;
	and.b32  	%r339, %r56, 1;
	shl.b32 	%r340, %r339, 3;
	setp.eq.s32	%p94, %r339, 0;
	selp.f64	%fd382, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p94;
	add.s32 	%r341, %r340, 1;
	mul.wide.s32 	%rd138, %r341, 8;
	add.s64 	%rd140, %rd135, %rd138;
	ld.const.f64 	%fd383, [%rd140];
	mul.rn.f64 	%fd86, %fd645, %fd645;
	fma.rn.f64 	%fd384, %fd382, %fd86, %fd383;
	ld.const.f64 	%fd385, [%rd140+8];
	fma.rn.f64 	%fd386, %fd384, %fd86, %fd385;
	ld.const.f64 	%fd387, [%rd140+16];
	fma.rn.f64 	%fd388, %fd386, %fd86, %fd387;
	ld.const.f64 	%fd389, [%rd140+24];
	fma.rn.f64 	%fd390, %fd388, %fd86, %fd389;
	ld.const.f64 	%fd391, [%rd140+32];
	fma.rn.f64 	%fd392, %fd390, %fd86, %fd391;
	ld.const.f64 	%fd393, [%rd140+40];
	fma.rn.f64 	%fd87, %fd392, %fd86, %fd393;
	fma.rn.f64 	%fd646, %fd87, %fd645, %fd645;
	@%p94 bra 	BB0_112;

	mov.f64 	%fd394, 0d3FF0000000000000;
	fma.rn.f64 	%fd646, %fd87, %fd86, %fd394;

BB0_112:
	and.b32  	%r342, %r56, 2;
	setp.eq.s32	%p95, %r342, 0;
	@%p95 bra 	BB0_114;

	mov.f64 	%fd395, 0d0000000000000000;
	mov.f64 	%fd396, 0dBFF0000000000000;
	fma.rn.f64 	%fd646, %fd646, %fd396, %fd395;

BB0_114:
	cvt.rn.f32.f64	%f91, %fd646;
	st.global.f32 	[%rd284+4], %f91;
	add.s32 	%r57, %r52, 1;
	cvt.rn.f64.s32	%fd397, %r57;
	mul.f64 	%fd398, %fd397, 0d400921FB54442D18;
	div.rn.f64 	%fd648, %fd398, %fd27;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r343}, %fd648;
	}
	and.b32  	%r344, %r343, 2147483647;
	setp.ne.s32	%p96, %r344, 2146435072;
	@%p96 bra 	BB0_117;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r345, %temp}, %fd648;
	}
	setp.ne.s32	%p97, %r345, 0;
	@%p97 bra 	BB0_117;

	mov.f64 	%fd399, 0d0000000000000000;
	mul.rn.f64 	%fd648, %fd648, %fd399;

BB0_117:
	mul.f64 	%fd400, %fd648, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r771, %fd400;
	st.local.u32 	[%rd7], %r771;
	cvt.rn.f64.s32	%fd401, %r771;
	neg.f64 	%fd402, %fd401;
	fma.rn.f64 	%fd404, %fd402, %fd351, %fd648;
	fma.rn.f64 	%fd406, %fd402, %fd353, %fd404;
	fma.rn.f64 	%fd649, %fd402, %fd355, %fd406;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r346}, %fd648;
	}
	and.b32  	%r347, %r346, 2145386496;
	setp.lt.u32	%p98, %r347, 1105199104;
	@%p98 bra 	BB0_119;

	// Callseq Start 7
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd648;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd73;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd649, [retval0+0];
	
	//{
	}// Callseq End 7
	ld.local.u32 	%r771, [%rd7];

BB0_119:
	add.s32 	%r61, %r771, 1;
	and.b32  	%r348, %r61, 1;
	shl.b32 	%r349, %r348, 3;
	setp.eq.s32	%p99, %r348, 0;
	selp.f64	%fd408, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p99;
	add.s32 	%r350, %r349, 1;
	mul.wide.s32 	%rd142, %r350, 8;
	add.s64 	%rd144, %rd135, %rd142;
	ld.const.f64 	%fd409, [%rd144];
	mul.rn.f64 	%fd99, %fd649, %fd649;
	fma.rn.f64 	%fd410, %fd408, %fd99, %fd409;
	ld.const.f64 	%fd411, [%rd144+8];
	fma.rn.f64 	%fd412, %fd410, %fd99, %fd411;
	ld.const.f64 	%fd413, [%rd144+16];
	fma.rn.f64 	%fd414, %fd412, %fd99, %fd413;
	ld.const.f64 	%fd415, [%rd144+24];
	fma.rn.f64 	%fd416, %fd414, %fd99, %fd415;
	ld.const.f64 	%fd417, [%rd144+32];
	fma.rn.f64 	%fd418, %fd416, %fd99, %fd417;
	ld.const.f64 	%fd419, [%rd144+40];
	fma.rn.f64 	%fd100, %fd418, %fd99, %fd419;
	fma.rn.f64 	%fd650, %fd100, %fd649, %fd649;
	@%p99 bra 	BB0_121;

	mov.f64 	%fd420, 0d3FF0000000000000;
	fma.rn.f64 	%fd650, %fd100, %fd99, %fd420;

BB0_121:
	and.b32  	%r351, %r61, 2;
	setp.eq.s32	%p100, %r351, 0;
	@%p100 bra 	BB0_123;

	mov.f64 	%fd421, 0d0000000000000000;
	mov.f64 	%fd422, 0dBFF0000000000000;
	fma.rn.f64 	%fd650, %fd650, %fd422, %fd421;

BB0_123:
	cvt.rn.f32.f64	%f92, %fd650;
	st.global.f32 	[%rd284+8], %f92;
	add.s32 	%r62, %r57, 1;
	cvt.rn.f64.s32	%fd423, %r62;
	mul.f64 	%fd424, %fd423, 0d400921FB54442D18;
	div.rn.f64 	%fd652, %fd424, %fd27;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r352}, %fd652;
	}
	and.b32  	%r353, %r352, 2147483647;
	setp.ne.s32	%p101, %r353, 2146435072;
	@%p101 bra 	BB0_126;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r354, %temp}, %fd652;
	}
	setp.ne.s32	%p102, %r354, 0;
	@%p102 bra 	BB0_126;

	mov.f64 	%fd425, 0d0000000000000000;
	mul.rn.f64 	%fd652, %fd652, %fd425;

BB0_126:
	mul.f64 	%fd426, %fd652, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r772, %fd426;
	st.local.u32 	[%rd7], %r772;
	cvt.rn.f64.s32	%fd427, %r772;
	neg.f64 	%fd428, %fd427;
	fma.rn.f64 	%fd430, %fd428, %fd351, %fd652;
	fma.rn.f64 	%fd432, %fd428, %fd353, %fd430;
	fma.rn.f64 	%fd653, %fd428, %fd355, %fd432;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r355}, %fd652;
	}
	and.b32  	%r356, %r355, 2145386496;
	setp.lt.u32	%p103, %r356, 1105199104;
	@%p103 bra 	BB0_128;

	// Callseq Start 8
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd652;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd73;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd653, [retval0+0];
	
	//{
	}// Callseq End 8
	ld.local.u32 	%r772, [%rd7];

BB0_128:
	add.s32 	%r66, %r772, 1;
	and.b32  	%r357, %r66, 1;
	shl.b32 	%r358, %r357, 3;
	setp.eq.s32	%p104, %r357, 0;
	selp.f64	%fd434, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p104;
	add.s32 	%r359, %r358, 1;
	mul.wide.s32 	%rd146, %r359, 8;
	add.s64 	%rd148, %rd135, %rd146;
	ld.const.f64 	%fd435, [%rd148];
	mul.rn.f64 	%fd112, %fd653, %fd653;
	fma.rn.f64 	%fd436, %fd434, %fd112, %fd435;
	ld.const.f64 	%fd437, [%rd148+8];
	fma.rn.f64 	%fd438, %fd436, %fd112, %fd437;
	ld.const.f64 	%fd439, [%rd148+16];
	fma.rn.f64 	%fd440, %fd438, %fd112, %fd439;
	ld.const.f64 	%fd441, [%rd148+24];
	fma.rn.f64 	%fd442, %fd440, %fd112, %fd441;
	ld.const.f64 	%fd443, [%rd148+32];
	fma.rn.f64 	%fd444, %fd442, %fd112, %fd443;
	ld.const.f64 	%fd445, [%rd148+40];
	fma.rn.f64 	%fd113, %fd444, %fd112, %fd445;
	fma.rn.f64 	%fd654, %fd113, %fd653, %fd653;
	@%p104 bra 	BB0_130;

	mov.f64 	%fd446, 0d3FF0000000000000;
	fma.rn.f64 	%fd654, %fd113, %fd112, %fd446;

BB0_130:
	and.b32  	%r360, %r66, 2;
	setp.eq.s32	%p105, %r360, 0;
	@%p105 bra 	BB0_132;

	mov.f64 	%fd447, 0d0000000000000000;
	mov.f64 	%fd448, 0dBFF0000000000000;
	fma.rn.f64 	%fd654, %fd654, %fd448, %fd447;

BB0_132:
	add.s64 	%rd18, %rd284, 16;
	cvt.rn.f32.f64	%f93, %fd654;
	st.global.f32 	[%rd284+12], %f93;
	add.s32 	%r763, %r62, 1;
	setp.lt.s32	%p106, %r763, %r227;
	mov.u64 	%rd284, %rd18;
	@%p106 bra 	BB0_96;

BB0_133:
	mul.wide.s32 	%rd149, %r227, 4;
	add.s64 	%rd150, %rd4, %rd149;
	mov.u32 	%r361, -1082130432;
	st.global.u32 	[%rd150], %r361;
	mov.u32 	%r69, %ntid.x;
	mov.u32 	%r362, %ctaid.x;
	mov.u32 	%r363, %tid.x;
	mad.lo.s32 	%r773, %r69, %r362, %r363;
	setp.ge.s32	%p107, %r773, %r221;
	@%p107 bra 	BB0_380;

	cvta.to.global.u64 	%rd19, %rd63;
	mov.u32 	%r364, %nctaid.x;
	mul.lo.s32 	%r71, %r364, %r69;
	cvt.rn.f64.s32	%fd119, %r227;
	mov.b64 	 %rd20, %fd119;
	shr.u32 	%r365, %r227, 31;
	add.s32 	%r366, %r227, %r365;
	shr.s32 	%r72, %r366, 1;
	and.b32  	%r73, %r227, 3;
	setp.eq.s32	%p108, %r73, 2;
	selp.b32	%r367, 1, %r227, %p108;
	setp.ne.s32	%p109, %r73, 2;
	selp.b32	%r368, -1, 0, %p109;
	add.s32 	%r369, %r368, %r227;
	add.s32 	%r74, %r369, -1;
	mul.lo.s32 	%r75, %r369, %r367;
	add.f64 	%fd120, %fd119, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r76}, %fd120;
	}
	bfe.u32 	%r370, %r76, 20, 11;
	add.s32 	%r371, %r370, -1012;
	mov.b64 	 %rd151, %fd120;
	shl.b64 	%rd21, %rd151, %r371;
	and.b32  	%r77, %r76, 2147483647;
	shr.s32 	%r372, %r76, 31;
	and.b32  	%r373, %r372, -2146435072;
	add.s32 	%r78, %r373, 2146435072;
	or.b32  	%r79, %r78, -2147483648;
	mov.f64 	%fd449, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r80}, %fd449;
	}
	and.b32  	%r81, %r80, 2147483647;
	and.b32  	%r82, %r225, 3;
	add.s64 	%rd22, %rd64, 12;
	add.s64 	%rd23, %rd63, 12;
	add.s64 	%rd24, %rd1, 12;
	add.s64 	%rd25, %rd2, 12;
	bra.uni 	BB0_135;

BB0_355:
	setp.ne.s32	%p339, %r81, 2146435072;
	@%p339 bra 	BB0_356;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r681, %temp}, %fd449;
	}
	setp.ne.s32	%p340, %r681, 0;
	mov.f64 	%fd692, %fd252;
	@%p340 bra 	BB0_360;

	shr.s32 	%r682, %r209, 31;
	and.b32  	%r683, %r682, -2146435072;
	add.s32 	%r684, %r683, 2146435072;
	or.b32  	%r685, %r684, -2147483648;
	selp.b32	%r686, %r685, %r684, %p14;
	mov.u32 	%r687, 0;
	mov.b64 	%fd692, {%r687, %r686};
	bra.uni 	BB0_360;

BB0_244:
	setp.ne.s32	%p221, %r81, 2146435072;
	@%p221 bra 	BB0_245;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r524, %temp}, %fd449;
	}
	setp.ne.s32	%p222, %r524, 0;
	mov.f64 	%fd672, %fd179;
	@%p222 bra 	BB0_249;

	shr.s32 	%r525, %r152, 31;
	and.b32  	%r526, %r525, -2146435072;
	add.s32 	%r527, %r526, 2146435072;
	or.b32  	%r528, %r527, -2147483648;
	selp.b32	%r529, %r528, %r527, %p7;
	mov.u32 	%r530, 0;
	mov.b64 	%fd672, {%r530, %r529};
	bra.uni 	BB0_249;

BB0_262:
	and.b32  	%r544, %r102, 2147483647;
	setp.ne.s32	%p236, %r544, 2146435072;
	@%p236 bra 	BB0_263;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r545, %temp}, %fd121;
	}
	setp.ne.s32	%p237, %r545, 0;
	mov.f64 	%fd675, %fd189;
	@%p237 bra 	BB0_267;

	selp.b32	%r546, %r79, %r78, %p8;
	mov.u32 	%r547, 0;
	mov.b64 	%fd675, {%r547, %r546};
	bra.uni 	BB0_267;

BB0_356:
	mov.f64 	%fd692, %fd252;
	bra.uni 	BB0_360;

BB0_165:
	and.b32  	%r403, %r102, 2147483647;
	setp.ne.s32	%p134, %r403, 2146435072;
	@%p134 bra 	BB0_166;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r404, %temp}, %fd121;
	}
	setp.ne.s32	%p135, %r404, 0;
	mov.f64 	%fd658, %fd128;
	@%p135 bra 	BB0_170;

	shr.s32 	%r405, %r103, 31;
	and.b32  	%r406, %r405, -2146435072;
	add.s32 	%r407, %r406, 2146435072;
	or.b32  	%r408, %r407, -2147483648;
	selp.b32	%r409, %r408, %r407, %p3;
	mov.u32 	%r410, 0;
	mov.b64 	%fd658, {%r410, %r409};
	bra.uni 	BB0_170;

BB0_245:
	mov.f64 	%fd672, %fd179;
	bra.uni 	BB0_249;

BB0_263:
	mov.f64 	%fd675, %fd189;
	bra.uni 	BB0_267;

BB0_166:
	mov.f64 	%fd658, %fd128;
	bra.uni 	BB0_170;

BB0_135:
	div.s32 	%r85, %r773, %r226;
	mul.wide.s32 	%rd152, %r85, 4;
	add.s64 	%rd27, %rd6, %rd152;
	ld.global.f32 	%f19, [%rd27];
	mul.lo.s32 	%r86, %r85, %r226;
	mul.lo.s32 	%r87, %r85, %r225;
	@%p40 bra 	BB0_148;

	mul.wide.s32 	%rd153, %r87, 4;
	add.s64 	%rd28, %rd63, %rd153;
	mov.u32 	%r774, 0;

BB0_137:
	mul.lo.s32 	%r89, %r774, %r225;
	mov.f32 	%f274, 0f00000000;
	setp.lt.s32	%p111, %r225, 1;
	@%p111 bra 	BB0_147;

	mov.f32 	%f274, 0f00000000;
	mov.u32 	%r777, 0;
	setp.eq.s32	%p112, %r82, 0;
	@%p112 bra 	BB0_144;

	setp.eq.s32	%p113, %r82, 1;
	@%p113 bra 	BB0_143;

	setp.eq.s32	%p114, %r82, 2;
	@%p114 bra 	BB0_142;

	// inline asm
	ld.global.nc.f32 %f98, [%rd28];
	// inline asm
	mul.wide.s32 	%rd156, %r89, 4;
	add.s64 	%rd155, %rd64, %rd156;
	// inline asm
	ld.global.nc.f32 %f99, [%rd155];
	// inline asm
	fma.rn.f32 	%f274, %f98, %f99, 0f00000000;
	mov.u32 	%r777, 1;

BB0_142:
	add.s32 	%r379, %r777, %r87;
	mul.wide.s32 	%rd159, %r379, 4;
	add.s64 	%rd157, %rd63, %rd159;
	// inline asm
	ld.global.nc.f32 %f100, [%rd157];
	// inline asm
	add.s32 	%r380, %r777, %r89;
	mul.wide.s32 	%rd160, %r380, 4;
	add.s64 	%rd158, %rd64, %rd160;
	// inline asm
	ld.global.nc.f32 %f101, [%rd158];
	// inline asm
	fma.rn.f32 	%f274, %f100, %f101, %f274;
	add.s32 	%r777, %r777, 1;

BB0_143:
	add.s32 	%r381, %r777, %r87;
	mul.wide.s32 	%rd163, %r381, 4;
	add.s64 	%rd161, %rd63, %rd163;
	// inline asm
	ld.global.nc.f32 %f102, [%rd161];
	// inline asm
	add.s32 	%r382, %r777, %r89;
	mul.wide.s32 	%rd164, %r382, 4;
	add.s64 	%rd162, %rd64, %rd164;
	// inline asm
	ld.global.nc.f32 %f103, [%rd162];
	// inline asm
	fma.rn.f32 	%f274, %f102, %f103, %f274;
	add.s32 	%r777, %r777, 1;

BB0_144:
	setp.lt.u32	%p115, %r225, 4;
	@%p115 bra 	BB0_147;

	mul.wide.s32 	%rd285, %r777, 4;
	mul.lo.s32 	%r383, %r225, %r774;
	mul.wide.s32 	%rd165, %r383, 4;
	add.s64 	%rd30, %rd22, %rd165;

BB0_146:
	add.s64 	%rd166, %rd28, %rd285;
	// inline asm
	ld.global.nc.f32 %f104, [%rd166];
	// inline asm
	add.s64 	%rd173, %rd30, %rd285;
	add.s64 	%rd167, %rd173, -12;
	// inline asm
	ld.global.nc.f32 %f105, [%rd167];
	// inline asm
	fma.rn.f32 	%f112, %f104, %f105, %f274;
	add.s64 	%rd168, %rd166, 4;
	// inline asm
	ld.global.nc.f32 %f106, [%rd168];
	// inline asm
	add.s64 	%rd169, %rd173, -8;
	// inline asm
	ld.global.nc.f32 %f107, [%rd169];
	// inline asm
	fma.rn.f32 	%f113, %f106, %f107, %f112;
	add.s64 	%rd170, %rd166, 8;
	// inline asm
	ld.global.nc.f32 %f108, [%rd170];
	// inline asm
	add.s64 	%rd171, %rd173, -4;
	// inline asm
	ld.global.nc.f32 %f109, [%rd171];
	// inline asm
	fma.rn.f32 	%f114, %f108, %f109, %f113;
	add.s64 	%rd172, %rd166, 12;
	// inline asm
	ld.global.nc.f32 %f110, [%rd172];
	// inline asm
	// inline asm
	ld.global.nc.f32 %f111, [%rd173];
	// inline asm
	fma.rn.f32 	%f274, %f110, %f111, %f114;
	add.s64 	%rd285, %rd285, 16;
	add.s32 	%r777, %r777, 4;
	setp.lt.s32	%p116, %r777, %r225;
	@%p116 bra 	BB0_146;

BB0_147:
	mul.wide.s32 	%rd174, %r774, 4;
	add.s64 	%rd175, %rd5, %rd174;
	ld.global.f32 	%f115, [%rd175];
	ld.global.f32 	%f116, [%rd27];
	mul.f32 	%f117, %f116, %f115;
	div.rn.f32 	%f118, %f274, %f117;
	add.s32 	%r384, %r774, %r86;
	mul.wide.s32 	%rd176, %r384, 4;
	add.s64 	%rd177, %rd3, %rd176;
	st.global.f32 	[%rd177], %f118;
	add.s32 	%r774, %r774, 1;
	setp.lt.s32	%p117, %r774, %r226;
	@%p117 bra 	BB0_137;

BB0_148:
	rem.s32 	%r724, %r773, %r226;
	add.s32 	%r387, %r86, %r724;
	mul.wide.s32 	%rd178, %r387, 4;
	add.s64 	%rd179, %rd3, %rd178;
	ld.global.f32 	%f29, [%rd179];
	mov.u32 	%r780, 0;
	mov.u32 	%r779, 1;
	@%p66 bra 	BB0_152;

BB0_149:
	mul.wide.s32 	%rd180, %r779, 4;
	add.s64 	%rd181, %rd4, %rd180;
	ld.global.f32 	%f119, [%rd181];
	setp.gt.f32	%p119, %f29, %f119;
	@%p119 bra 	BB0_151;

	add.s32 	%r99, %r779, 1;
	setp.lt.s32	%p120, %r779, %r227;
	mov.u32 	%r779, %r99;
	@%p120 bra 	BB0_149;
	bra.uni 	BB0_152;

BB0_151:
	add.s32 	%r780, %r779, -1;

BB0_152:
	ld.param.u64 	%rd280, [_Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_5];
	div.s32 	%r726, %r773, %r226;
	mul.wide.s32 	%rd279, %r726, 4;
	rem.s32 	%r725, %r773, %r226;
	mul.f32 	%f121, %f29, %f29;
	mov.f32 	%f122, 0f3F800000;
	sub.f32 	%f30, %f122, %f121;
	add.s64 	%rd182, %rd280, %rd279;
	// inline asm
	ld.global.nc.s32 %r389, [%rd182];
	// inline asm
	cvt.f64.f32	%fd121, %f29;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r102}, %fd121;
	}
	abs.f64 	%fd122, %fd121;
	mov.f32 	%f275, 0f00000000;
	setp.ne.s32	%p121, %r389, %r725;
	@%p121 bra 	BB0_250;

	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r103}, %fd119;
	}
	bfe.u32 	%r390, %r103, 20, 11;
	add.s32 	%r391, %r390, -1012;
	shl.b64 	%rd34, %rd20, %r391;
	setp.eq.s64	%p122, %rd34, -9223372036854775808;
	// Callseq Start 9
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd122;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd119;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd128, [retval0+0];
	
	//{
	}// Callseq End 9
	setp.lt.s32	%p123, %r102, 0;
	and.pred  	%p3, %p123, %p122;
	@!%p3 bra 	BB0_155;
	bra.uni 	BB0_154;

BB0_154:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r392}, %fd128;
	}
	xor.b32  	%r393, %r392, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r394, %temp}, %fd128;
	}
	mov.b64 	%fd128, {%r394, %r393};

BB0_155:
	setp.eq.f32	%p124, %f29, 0f00000000;
	@%p124 bra 	BB0_158;
	bra.uni 	BB0_156;

BB0_158:
	selp.b32	%r395, %r102, 0, %p122;
	or.b32  	%r396, %r395, 2146435072;
	setp.lt.s32	%p128, %r103, 0;
	selp.b32	%r397, %r396, %r395, %p128;
	mov.u32 	%r398, 0;
	mov.b64 	%fd128, {%r398, %r397};
	bra.uni 	BB0_159;

BB0_156:
	setp.gt.s32	%p125, %r102, -1;
	@%p125 bra 	BB0_159;

	cvt.rzi.f64.f64	%fd450, %fd119;
	setp.neu.f64	%p126, %fd450, %fd119;
	selp.f64	%fd128, 0dFFF8000000000000, %fd128, %p126;

BB0_159:
	add.f64 	%fd658, %fd119, %fd121;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r399}, %fd658;
	}
	and.b32  	%r400, %r399, 2146435072;
	setp.ne.s32	%p129, %r400, 2146435072;
	@%p129 bra 	BB0_160;

	setp.gtu.f64	%p130, %fd122, 0d7FF0000000000000;
	@%p130 bra 	BB0_170;

	abs.f64 	%fd451, %fd119;
	setp.gtu.f64	%p131, %fd451, 0d7FF0000000000000;
	@%p131 bra 	BB0_170;

	and.b32  	%r401, %r103, 2147483647;
	setp.ne.s32	%p132, %r401, 2146435072;
	@%p132 bra 	BB0_165;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r402, %temp}, %fd119;
	}
	setp.eq.s32	%p133, %r402, 0;
	@%p133 bra 	BB0_169;
	bra.uni 	BB0_165;

BB0_169:
	setp.gt.f64	%p136, %fd122, 0d3FF0000000000000;
	selp.b32	%r411, 2146435072, 0, %p136;
	xor.b32  	%r412, %r411, 2146435072;
	setp.lt.s32	%p137, %r103, 0;
	selp.b32	%r413, %r412, %r411, %p137;
	setp.eq.f32	%p138, %f29, 0fBF800000;
	selp.b32	%r414, 1072693248, %r413, %p138;
	mov.u32 	%r415, 0;
	mov.b64 	%fd658, {%r415, %r414};
	bra.uni 	BB0_170;

BB0_160:
	mov.f64 	%fd658, %fd128;

BB0_170:
	setp.eq.f32	%p139, %f29, 0f3F800000;
	setp.eq.s32	%p140, %r227, 0;
	or.pred  	%p141, %p139, %p140;
	cvt.rn.f32.f64	%f123, %fd658;
	cvt.f64.f32	%fd452, %f123;
	selp.f64	%fd669, 0d3FF0000000000000, %fd452, %p141;
	setp.lt.s32	%p142, %r227, 2;
	@%p142 bra 	BB0_235;

	cvt.f64.f32	%fd134, %f30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r104}, %fd134;
	}
	abs.f64 	%fd135, %fd134;
	setp.gt.f64	%p143, %fd135, 0d3FF0000000000000;
	selp.b32	%r105, 2146435072, 0, %p143;
	xor.b32  	%r106, %r105, 2146435072;
	and.b32  	%r107, %r104, 2147483647;
	setp.gt.f64	%p144, %fd122, 0d3FF0000000000000;
	mov.u32 	%r781, 1;

BB0_172:
	mov.u32 	%r111, %r781;
	mov.u32 	%r789, 1;
	@%p66 bra 	BB0_180;

	mov.u32 	%r787, 1;
	mov.u32 	%r789, 0;
	setp.eq.s32	%p146, %r73, 0;
	@%p146 bra 	BB0_174;
	bra.uni 	BB0_175;

BB0_174:
	mov.u32 	%r788, %r227;
	bra.uni 	BB0_178;

BB0_175:
	setp.eq.s32	%p147, %r73, 1;
	mov.u32 	%r783, %r227;
	@%p147 bra 	BB0_177;

	mov.u32 	%r787, %r75;
	mov.u32 	%r783, %r74;

BB0_177:
	add.s32 	%r788, %r783, -1;
	mul.lo.s32 	%r787, %r783, %r787;
	mov.u32 	%r789, %r787;

BB0_178:
	setp.lt.u32	%p148, %r227, 4;
	@%p148 bra 	BB0_180;

BB0_179:
	mul.lo.s32 	%r421, %r788, %r787;
	add.s32 	%r422, %r788, -1;
	mul.lo.s32 	%r423, %r421, %r422;
	add.s32 	%r424, %r788, -2;
	mul.lo.s32 	%r425, %r423, %r424;
	add.s32 	%r426, %r788, -3;
	mul.lo.s32 	%r787, %r425, %r426;
	add.s32 	%r788, %r788, -4;
	setp.gt.s32	%p149, %r788, 0;
	mov.u32 	%r789, %r787;
	@%p149 bra 	BB0_179;

BB0_180:
	mov.u32 	%r792, 1;
	shl.b32 	%r124, %r111, 1;
	setp.lt.s32	%p150, %r111, 1;
	@%p150 bra 	BB0_183;

	bfe.s32 	%r428, %r111, 0, 1;
	and.b32  	%r429, %r111, 1;
	setp.eq.b32	%p151, %r429, 1;
	not.pred 	%p152, %p151;
	add.s32 	%r430, %r124, -1;
	mul.lo.s32 	%r431, %r124, %r430;
	selp.b32	%r790, 1, %r431, %p152;
	add.s32 	%r432, %r124, -2;
	selp.b32	%r791, %r124, %r432, %p152;
	and.b32  	%r792, %r428, %r431;
	setp.lt.u32	%p153, %r124, 4;
	@%p153 bra 	BB0_183;

BB0_182:
	mul.lo.s32 	%r433, %r791, %r790;
	add.s32 	%r434, %r791, -1;
	mul.lo.s32 	%r435, %r433, %r434;
	add.s32 	%r436, %r791, -2;
	mul.lo.s32 	%r437, %r435, %r436;
	add.s32 	%r438, %r791, -3;
	mul.lo.s32 	%r790, %r437, %r438;
	add.s32 	%r791, %r791, -4;
	setp.gt.s32	%p154, %r791, 0;
	mov.u32 	%r792, %r790;
	@%p154 bra 	BB0_182;

BB0_183:
	mov.u32 	%r800, 1;
	sub.s32 	%r133, %r227, %r124;
	setp.lt.s32	%p155, %r133, 1;
	@%p155 bra 	BB0_190;

	and.b32  	%r134, %r133, 3;
	setp.eq.s32	%p156, %r134, 0;
	mov.u32 	%r800, 0;
	mov.u32 	%r798, 1;
	mov.u32 	%r799, %r133;
	@%p156 bra 	BB0_188;

	setp.eq.s32	%p157, %r134, 1;
	mov.u32 	%r793, 1;
	mov.u32 	%r794, %r133;
	@%p157 bra 	BB0_187;

	setp.eq.s32	%p158, %r134, 2;
	selp.b32	%r443, 1, %r133, %p158;
	setp.ne.s32	%p159, %r134, 2;
	selp.b32	%r444, -1, 0, %p159;
	add.s32 	%r445, %r444, %r133;
	add.s32 	%r794, %r445, -1;
	mul.lo.s32 	%r793, %r445, %r443;

BB0_187:
	add.s32 	%r799, %r794, -1;
	mul.lo.s32 	%r798, %r794, %r793;
	mov.u32 	%r800, %r798;

BB0_188:
	setp.lt.u32	%p160, %r133, 4;
	@%p160 bra 	BB0_190;

BB0_189:
	mul.lo.s32 	%r446, %r799, %r798;
	add.s32 	%r447, %r799, -1;
	mul.lo.s32 	%r448, %r446, %r447;
	add.s32 	%r449, %r799, -2;
	mul.lo.s32 	%r450, %r448, %r449;
	add.s32 	%r451, %r799, -3;
	mul.lo.s32 	%r798, %r450, %r451;
	add.s32 	%r799, %r799, -4;
	setp.gt.s32	%p161, %r799, 0;
	mov.u32 	%r800, %r798;
	@%p161 bra 	BB0_189;

BB0_190:
	cvt.rn.f64.s32	%fd453, %r789;
	mul.lo.s32 	%r452, %r800, %r792;
	cvt.rn.f64.s32	%fd454, %r452;
	div.rn.f64 	%fd137, %fd453, %fd454;
	cvt.rn.f64.s32	%fd138, %r111;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r149}, %fd138;
	}
	bfe.u32 	%r453, %r149, 20, 11;
	add.s32 	%r454, %r453, -1012;
	mov.b64 	 %rd184, %fd138;
	shl.b64 	%rd35, %rd184, %r454;
	setp.eq.s64	%p162, %rd35, -9223372036854775808;
	mov.f64 	%fd455, 0d3FF0000000000000;
	// Callseq Start 10
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd455;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd138;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd143, [retval0+0];
	
	//{
	}// Callseq End 10
	setp.lt.s32	%p163, %r80, 0;
	and.pred  	%p4, %p163, %p162;
	@!%p4 bra 	BB0_192;
	bra.uni 	BB0_191;

BB0_191:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r455}, %fd143;
	}
	xor.b32  	%r456, %r455, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r457, %temp}, %fd143;
	}
	mov.b64 	%fd143, {%r457, %r456};

BB0_192:
	setp.gt.s32	%p164, %r80, -1;
	@%p164 bra 	BB0_194;

	cvt.rzi.f64.f64	%fd456, %fd138;
	setp.neu.f64	%p165, %fd456, %fd138;
	selp.f64	%fd143, 0dFFF8000000000000, %fd143, %p165;

BB0_194:
	add.f64 	%fd662, %fd138, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r458}, %fd662;
	}
	and.b32  	%r459, %r458, 2146435072;
	setp.ne.s32	%p166, %r459, 2146435072;
	@%p166 bra 	BB0_195;

	abs.f64 	%fd457, %fd138;
	setp.gtu.f64	%p167, %fd457, 0d7FF0000000000000;
	@%p167 bra 	BB0_204;

	and.b32  	%r460, %r149, 2147483647;
	setp.ne.s32	%p168, %r460, 2146435072;
	@%p168 bra 	BB0_199;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r461, %temp}, %fd138;
	}
	setp.eq.s32	%p169, %r461, 0;
	@%p169 bra 	BB0_203;
	bra.uni 	BB0_199;

BB0_203:
	mov.u32 	%r469, 1072693248;
	mov.u32 	%r470, 0;
	mov.b64 	%fd662, {%r470, %r469};
	bra.uni 	BB0_204;

BB0_195:
	mov.f64 	%fd662, %fd143;

BB0_204:
	// Callseq Start 11
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd135;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd138;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd664, [retval0+0];
	
	//{
	}// Callseq End 11
	setp.lt.s32	%p172, %r104, 0;
	and.pred  	%p5, %p172, %p162;
	@!%p5 bra 	BB0_206;
	bra.uni 	BB0_205;

BB0_205:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r471}, %fd664;
	}
	xor.b32  	%r472, %r471, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r473, %temp}, %fd664;
	}
	mov.b64 	%fd664, {%r473, %r472};

BB0_206:
	setp.eq.f32	%p174, %f30, 0f00000000;
	@%p174 bra 	BB0_209;
	bra.uni 	BB0_207;

BB0_209:
	selp.b32	%r474, %r104, 0, %p162;
	or.b32  	%r475, %r474, 2146435072;
	setp.lt.s32	%p178, %r149, 0;
	selp.b32	%r476, %r475, %r474, %p178;
	mov.u32 	%r477, 0;
	mov.b64 	%fd664, {%r477, %r476};
	bra.uni 	BB0_210;

BB0_207:
	setp.gt.s32	%p175, %r104, -1;
	@%p175 bra 	BB0_210;

	cvt.rzi.f64.f64	%fd459, %fd138;
	setp.neu.f64	%p176, %fd459, %fd138;
	selp.f64	%fd664, 0dFFF8000000000000, %fd664, %p176;

BB0_210:
	add.f64 	%fd154, %fd134, %fd138;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r478}, %fd154;
	}
	and.b32  	%r479, %r478, 2146435072;
	setp.ne.s32	%p179, %r479, 2146435072;
	setp.gtu.f64	%p180, %fd135, 0d7FF0000000000000;
	or.pred  	%p181, %p179, %p180;
	selp.f64	%fd665, %fd664, %fd154, %p179;
	@%p181 bra 	BB0_219;

	abs.f64 	%fd460, %fd138;
	setp.gtu.f64	%p182, %fd460, 0d7FF0000000000000;
	mov.f64 	%fd665, %fd154;
	@%p182 bra 	BB0_219;

	and.b32  	%r480, %r149, 2147483647;
	setp.ne.s32	%p183, %r480, 2146435072;
	@%p183 bra 	BB0_214;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r481, %temp}, %fd138;
	}
	setp.eq.s32	%p184, %r481, 0;
	@%p184 bra 	BB0_218;
	bra.uni 	BB0_214;

BB0_218:
	setp.eq.f32	%p187, %f30, 0fBF800000;
	setp.lt.s32	%p188, %r149, 0;
	selp.b32	%r489, %r106, %r105, %p188;
	selp.b32	%r490, 1072693248, %r489, %p187;
	mov.u32 	%r491, 0;
	mov.b64 	%fd665, {%r491, %r490};
	bra.uni 	BB0_219;

BB0_199:
	setp.ne.s32	%p170, %r81, 2146435072;
	@%p170 bra 	BB0_200;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r462, %temp}, %fd449;
	}
	setp.ne.s32	%p171, %r462, 0;
	mov.f64 	%fd662, %fd143;
	@%p171 bra 	BB0_204;

	shr.s32 	%r463, %r149, 31;
	and.b32  	%r464, %r463, -2146435072;
	add.s32 	%r465, %r464, 2146435072;
	or.b32  	%r466, %r465, -2147483648;
	selp.b32	%r467, %r466, %r465, %p4;
	mov.u32 	%r468, 0;
	mov.b64 	%fd662, {%r468, %r467};
	bra.uni 	BB0_204;

BB0_214:
	setp.ne.s32	%p185, %r107, 2146435072;
	@%p185 bra 	BB0_215;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r482, %temp}, %fd134;
	}
	setp.ne.s32	%p186, %r482, 0;
	mov.f64 	%fd665, %fd664;
	@%p186 bra 	BB0_219;

	shr.s32 	%r483, %r149, 31;
	and.b32  	%r484, %r483, -2146435072;
	add.s32 	%r485, %r484, 2146435072;
	or.b32  	%r486, %r485, -2147483648;
	selp.b32	%r487, %r486, %r485, %p5;
	mov.u32 	%r488, 0;
	mov.b64 	%fd665, {%r488, %r487};
	bra.uni 	BB0_219;

BB0_200:
	mov.f64 	%fd662, %fd143;
	bra.uni 	BB0_204;

BB0_215:
	mov.f64 	%fd665, %fd664;

BB0_219:
	setp.lt.s32	%p366, %r102, 0;
	setp.eq.s32	%p189, %r111, 0;
	selp.f64	%fd461, 0d3FF0000000000000, %fd662, %p189;
	setp.eq.f32	%p190, %f30, 0f3F800000;
	or.pred  	%p191, %p190, %p189;
	selp.f64	%fd462, 0d3FF0000000000000, %fd665, %p191;
	mul.f64 	%fd159, %fd461, %fd462;
	fma.rn.f64 	%fd160, %fd138, 0dC000000000000000, %fd119;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r150}, %fd160;
	}
	bfe.u32 	%r492, %r150, 20, 11;
	add.s32 	%r493, %r492, -1012;
	mov.b64 	 %rd185, %fd160;
	shl.b64 	%rd36, %rd185, %r493;
	setp.eq.s64	%p192, %rd36, -9223372036854775808;
	// Callseq Start 12
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd122;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd160;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd667, [retval0+0];
	
	//{
	}// Callseq End 12
	and.pred  	%p6, %p366, %p192;
	@!%p6 bra 	BB0_221;
	bra.uni 	BB0_220;

BB0_220:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r494}, %fd667;
	}
	xor.b32  	%r495, %r494, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r496, %temp}, %fd667;
	}
	mov.b64 	%fd667, {%r496, %r495};

BB0_221:
	setp.eq.f32	%p367, %f29, 0f00000000;
	@%p367 bra 	BB0_224;
	bra.uni 	BB0_222;

BB0_224:
	selp.b32	%r497, %r102, 0, %p192;
	or.b32  	%r498, %r497, 2146435072;
	setp.lt.s32	%p198, %r150, 0;
	selp.b32	%r499, %r498, %r497, %p198;
	mov.u32 	%r500, 0;
	mov.b64 	%fd667, {%r500, %r499};
	bra.uni 	BB0_225;

BB0_222:
	setp.gt.s32	%p195, %r102, -1;
	@%p195 bra 	BB0_225;

	cvt.rzi.f64.f64	%fd463, %fd160;
	setp.neu.f64	%p196, %fd463, %fd160;
	selp.f64	%fd667, 0dFFF8000000000000, %fd667, %p196;

BB0_225:
	add.f64 	%fd167, %fd121, %fd160;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r501}, %fd167;
	}
	and.b32  	%r502, %r501, 2146435072;
	setp.ne.s32	%p199, %r502, 2146435072;
	setp.gtu.f64	%p200, %fd122, 0d7FF0000000000000;
	or.pred  	%p201, %p199, %p200;
	selp.f64	%fd668, %fd667, %fd167, %p199;
	@%p201 bra 	BB0_234;

	abs.f64 	%fd464, %fd160;
	setp.gtu.f64	%p202, %fd464, 0d7FF0000000000000;
	mov.f64 	%fd668, %fd167;
	@%p202 bra 	BB0_234;

	and.b32  	%r503, %r150, 2147483647;
	setp.ne.s32	%p203, %r503, 2146435072;
	@%p203 bra 	BB0_229;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r504, %temp}, %fd160;
	}
	setp.eq.s32	%p204, %r504, 0;
	@%p204 bra 	BB0_233;
	bra.uni 	BB0_229;

BB0_233:
	selp.b32	%r748, 2146435072, 0, %p144;
	setp.gt.f64	%p368, %fd122, 0d3FF0000000000000;
	selp.b32	%r747, 2146435072, 0, %p368;
	xor.b32  	%r746, %r747, 2146435072;
	setp.eq.f32	%p207, %f29, 0fBF800000;
	setp.lt.s32	%p208, %r150, 0;
	selp.b32	%r512, %r746, %r747, %p208;
	selp.b32	%r513, 1072693248, %r512, %p207;
	mov.u32 	%r514, 0;
	mov.b64 	%fd668, {%r514, %r513};
	bra.uni 	BB0_234;

BB0_229:
	and.b32  	%r745, %r102, 2147483647;
	setp.ne.s32	%p205, %r745, 2146435072;
	@%p205 bra 	BB0_230;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r505, %temp}, %fd121;
	}
	setp.ne.s32	%p206, %r505, 0;
	mov.f64 	%fd668, %fd667;
	@%p206 bra 	BB0_234;

	setp.lt.s32	%p371, %r102, 0;
	and.pred  	%p370, %p371, %p192;
	shr.s32 	%r506, %r150, 31;
	and.b32  	%r507, %r506, -2146435072;
	add.s32 	%r508, %r507, 2146435072;
	or.b32  	%r509, %r508, -2147483648;
	selp.b32	%r510, %r509, %r508, %p370;
	mov.u32 	%r511, 0;
	mov.b64 	%fd668, {%r511, %r510};
	bra.uni 	BB0_234;

BB0_230:
	mov.f64 	%fd668, %fd667;

BB0_234:
	cvt.rn.f32.f64	%f259, %fd137;
	setp.eq.f32	%p369, %f29, 0f3F800000;
	setp.eq.f64	%p209, %fd160, 0d0000000000000000;
	or.pred  	%p211, %p369, %p209;
	selp.f64	%fd465, 0d3FF0000000000000, %fd668, %p211;
	mul.f64 	%fd466, %fd159, %fd465;
	cvt.f64.f32	%fd467, %f259;
	fma.rn.f64 	%fd468, %fd467, %fd466, %fd669;
	add.s32 	%r781, %r111, 1;
	cvt.rn.f32.f64	%f124, %fd468;
	cvt.f64.f32	%fd669, %f124;
	setp.lt.s32	%p212, %r111, %r72;
	@%p212 bra 	BB0_172;

BB0_235:
	cvt.rn.f64.s32	%fd174, %r780;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r152}, %fd174;
	}
	bfe.u32 	%r515, %r152, 20, 11;
	add.s32 	%r516, %r515, -1012;
	mov.b64 	 %rd186, %fd174;
	shl.b64 	%rd187, %rd186, %r516;
	setp.eq.s64	%p213, %rd187, -9223372036854775808;
	mov.f64 	%fd469, 0d3FF0000000000000;
	// Callseq Start 13
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd469;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd174;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd179, [retval0+0];
	
	//{
	}// Callseq End 13
	setp.lt.s32	%p214, %r80, 0;
	and.pred  	%p7, %p214, %p213;
	@!%p7 bra 	BB0_237;
	bra.uni 	BB0_236;

BB0_236:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r517}, %fd179;
	}
	xor.b32  	%r518, %r517, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r519, %temp}, %fd179;
	}
	mov.b64 	%fd179, {%r519, %r518};

BB0_237:
	setp.gt.s32	%p215, %r80, -1;
	@%p215 bra 	BB0_239;

	cvt.rzi.f64.f64	%fd470, %fd174;
	setp.neu.f64	%p216, %fd470, %fd174;
	selp.f64	%fd179, 0dFFF8000000000000, %fd179, %p216;

BB0_239:
	add.f64 	%fd672, %fd174, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r520}, %fd672;
	}
	and.b32  	%r521, %r520, 2146435072;
	setp.ne.s32	%p217, %r521, 2146435072;
	@%p217 bra 	BB0_240;

	abs.f64 	%fd471, %fd174;
	setp.gtu.f64	%p218, %fd471, 0d7FF0000000000000;
	@%p218 bra 	BB0_249;

	and.b32  	%r522, %r152, 2147483647;
	setp.ne.s32	%p219, %r522, 2146435072;
	@%p219 bra 	BB0_244;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r523, %temp}, %fd174;
	}
	setp.eq.s32	%p220, %r523, 0;
	@%p220 bra 	BB0_248;
	bra.uni 	BB0_244;

BB0_248:
	mov.u32 	%r531, 1072693248;
	mov.u32 	%r532, 0;
	mov.b64 	%fd672, {%r532, %r531};
	bra.uni 	BB0_249;

BB0_240:
	mov.f64 	%fd672, %fd179;

BB0_249:
	setp.eq.s32	%p223, %r780, 0;
	selp.f64	%fd473, 0d3FF0000000000000, %fd672, %p223;
	mul.f64 	%fd474, %fd669, %fd473;
	shl.b32 	%r533, %r780, 1;
	cvt.rn.f64.s32	%fd475, %r533;
	sub.f64 	%fd476, %fd474, %fd475;
	cvt.rn.f32.f64	%f275, %fd476;

BB0_250:
	// Callseq Start 14
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd122;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd120;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd189, [retval0+0];
	
	//{
	}// Callseq End 14
	setp.lt.s32	%p224, %r102, 0;
	setp.eq.s64	%p225, %rd21, -9223372036854775808;
	and.pred  	%p8, %p224, %p225;
	@!%p8 bra 	BB0_252;
	bra.uni 	BB0_251;

BB0_251:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r534}, %fd189;
	}
	xor.b32  	%r535, %r534, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r536, %temp}, %fd189;
	}
	mov.b64 	%fd189, {%r536, %r535};

BB0_252:
	setp.eq.f32	%p226, %f29, 0f00000000;
	@%p226 bra 	BB0_255;
	bra.uni 	BB0_253;

BB0_255:
	setp.lt.s32	%p229, %r76, 0;
	selp.b32	%r537, %r102, 0, %p225;
	or.b32  	%r538, %r537, 2146435072;
	selp.b32	%r539, %r538, %r537, %p229;
	mov.u32 	%r540, 0;
	mov.b64 	%fd189, {%r540, %r539};
	bra.uni 	BB0_256;

BB0_253:
	setp.gt.s32	%p227, %r102, -1;
	@%p227 bra 	BB0_256;

	cvt.rzi.f64.f64	%fd477, %fd120;
	setp.neu.f64	%p228, %fd477, %fd120;
	selp.f64	%fd189, 0dFFF8000000000000, %fd189, %p228;

BB0_256:
	add.f64 	%fd675, %fd120, %fd121;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r541}, %fd675;
	}
	and.b32  	%r542, %r541, 2146435072;
	setp.ne.s32	%p231, %r542, 2146435072;
	@%p231 bra 	BB0_257;

	setp.gtu.f64	%p232, %fd122, 0d7FF0000000000000;
	@%p232 bra 	BB0_267;

	abs.f64 	%fd478, %fd120;
	setp.gtu.f64	%p233, %fd478, 0d7FF0000000000000;
	@%p233 bra 	BB0_267;

	setp.ne.s32	%p234, %r77, 2146435072;
	@%p234 bra 	BB0_262;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r543, %temp}, %fd120;
	}
	setp.eq.s32	%p235, %r543, 0;
	@%p235 bra 	BB0_266;
	bra.uni 	BB0_262;

BB0_266:
	setp.lt.s32	%p238, %r76, 0;
	setp.gt.f64	%p239, %fd122, 0d3FF0000000000000;
	selp.b32	%r548, 2146435072, 0, %p239;
	xor.b32  	%r549, %r548, 2146435072;
	selp.b32	%r550, %r549, %r548, %p238;
	setp.eq.f32	%p240, %f29, 0fBF800000;
	selp.b32	%r551, 1072693248, %r550, %p240;
	mov.u32 	%r552, 0;
	mov.b64 	%fd675, {%r552, %r551};
	bra.uni 	BB0_267;

BB0_257:
	mov.f64 	%fd675, %fd189;

BB0_267:
	setp.eq.f32	%p241, %f29, 0f3F800000;
	setp.eq.f64	%p242, %fd120, 0d0000000000000000;
	or.pred  	%p243, %p241, %p242;
	selp.f64	%fd479, 0d3FF0000000000000, %fd675, %p243;
	mul.f64 	%fd480, %fd119, %fd479;
	cvt.rn.f32.f64	%f125, %fd480;
	cvt.f64.f32	%fd689, %f125;
	setp.lt.s32	%p244, %r227, 2;
	@%p244 bra 	BB0_346;

	cvt.f64.f32	%fd617, %f29;
	setp.lt.s32	%p363, %r102, 0;
	cvt.f64.f32	%fd195, %f30;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r154}, %fd195;
	}
	abs.f64 	%fd196, %fd195;
	setp.gt.f64	%p246, %fd196, 0d3FF0000000000000;
	selp.b32	%r155, 2146435072, 0, %p246;
	xor.b32  	%r156, %r155, 2146435072;
	and.b32  	%r157, %r154, 2147483647;
	setp.gt.f64	%p247, %fd122, 0d3FF0000000000000;
	selp.b32	%r158, 2146435072, 0, %p247;
	xor.b32  	%r159, %r158, 2146435072;
	mov.f64 	%fd481, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r554}, %fd481;
	}
	bfe.u32 	%r555, %r554, 20, 11;
	add.s32 	%r556, %r555, -1012;
	mov.u64 	%rd188, 4611686018427387904;
	shl.b64 	%rd189, %rd188, %r556;
	setp.eq.s64	%p248, %rd189, -9223372036854775808;
	// Callseq Start 15
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd122;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd481;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd197, [retval0+0];
	
	//{
	}// Callseq End 15
	and.pred  	%p9, %p363, %p248;
	selp.b32	%r557, %r102, 0, %p248;
	setp.lt.s32	%p249, %r554, 0;
	or.b32  	%r558, %r557, 2146435072;
	selp.b32	%r161, %r558, %r557, %p249;
	add.f64 	%fd198, %fd617, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r559}, %fd198;
	}
	and.b32  	%r162, %r559, 2146435072;
	selp.b32	%r560, %r159, %r158, %p249;
	setp.eq.f32	%p250, %f29, 0fBF800000;
	selp.b32	%r164, 1072693248, %r560, %p250;
	setp.ne.s32	%p251, %r162, 2146435072;
	setp.gtu.f64	%p252, %fd122, 0d7FF0000000000000;
	or.pred  	%p10, %p251, %p252;
	mov.u32 	%r801, 1;

BB0_269:
	mov.u32 	%r166, %r801;
	mov.u32 	%r809, 1;
	@%p66 bra 	BB0_277;

	mov.u32 	%r807, 1;
	mov.u32 	%r809, 0;
	setp.eq.s32	%p254, %r73, 0;
	@%p254 bra 	BB0_271;
	bra.uni 	BB0_272;

BB0_271:
	mov.u32 	%r808, %r227;
	bra.uni 	BB0_275;

BB0_272:
	setp.eq.s32	%p255, %r73, 1;
	mov.u32 	%r803, %r227;
	@%p255 bra 	BB0_274;

	mov.u32 	%r807, %r75;
	mov.u32 	%r803, %r74;

BB0_274:
	add.s32 	%r808, %r803, -1;
	mul.lo.s32 	%r807, %r803, %r807;
	mov.u32 	%r809, %r807;

BB0_275:
	setp.lt.u32	%p256, %r227, 4;
	@%p256 bra 	BB0_277;

BB0_276:
	mul.lo.s32 	%r569, %r808, %r807;
	add.s32 	%r570, %r808, -1;
	mul.lo.s32 	%r571, %r569, %r570;
	add.s32 	%r572, %r808, -2;
	mul.lo.s32 	%r573, %r571, %r572;
	add.s32 	%r574, %r808, -3;
	mul.lo.s32 	%r807, %r573, %r574;
	add.s32 	%r808, %r808, -4;
	setp.gt.s32	%p257, %r808, 0;
	mov.u32 	%r809, %r807;
	@%p257 bra 	BB0_276;

BB0_277:
	mov.u32 	%r812, 1;
	shl.b32 	%r179, %r166, 1;
	setp.lt.s32	%p258, %r166, 1;
	@%p258 bra 	BB0_280;

	bfe.s32 	%r576, %r166, 0, 1;
	and.b32  	%r577, %r166, 1;
	setp.eq.b32	%p259, %r577, 1;
	not.pred 	%p260, %p259;
	add.s32 	%r578, %r179, -1;
	mul.lo.s32 	%r579, %r179, %r578;
	selp.b32	%r810, 1, %r579, %p260;
	add.s32 	%r580, %r179, -2;
	selp.b32	%r811, %r179, %r580, %p260;
	and.b32  	%r812, %r576, %r579;
	setp.lt.u32	%p261, %r179, 4;
	@%p261 bra 	BB0_280;

BB0_279:
	mul.lo.s32 	%r581, %r811, %r810;
	add.s32 	%r582, %r811, -1;
	mul.lo.s32 	%r583, %r581, %r582;
	add.s32 	%r584, %r811, -2;
	mul.lo.s32 	%r585, %r583, %r584;
	add.s32 	%r586, %r811, -3;
	mul.lo.s32 	%r810, %r585, %r586;
	add.s32 	%r811, %r811, -4;
	setp.gt.s32	%p262, %r811, 0;
	mov.u32 	%r812, %r810;
	@%p262 bra 	BB0_279;

BB0_280:
	shl.b32 	%r734, %r166, 1;
	mov.u32 	%r820, 1;
	sub.s32 	%r188, %r227, %r734;
	setp.lt.s32	%p263, %r188, 1;
	@%p263 bra 	BB0_287;

	shl.b32 	%r736, %r166, 1;
	sub.s32 	%r819, %r227, %r734;
	and.b32  	%r189, %r819, 3;
	setp.eq.s32	%p264, %r189, 0;
	mov.u32 	%r820, 0;
	mov.u32 	%r818, 1;
	@%p264 bra 	BB0_285;

	shl.b32 	%r738, %r166, 1;
	sub.s32 	%r814, %r227, %r734;
	setp.eq.s32	%p265, %r189, 1;
	mov.u32 	%r813, 1;
	@%p265 bra 	BB0_284;

	shl.b32 	%r740, %r166, 1;
	sub.s32 	%r739, %r227, %r734;
	setp.eq.s32	%p266, %r189, 2;
	selp.b32	%r591, 1, %r739, %p266;
	setp.ne.s32	%p267, %r189, 2;
	selp.b32	%r592, -1, 0, %p267;
	add.s32 	%r593, %r592, %r739;
	add.s32 	%r814, %r593, -1;
	mul.lo.s32 	%r813, %r593, %r591;

BB0_284:
	add.s32 	%r819, %r814, -1;
	mul.lo.s32 	%r818, %r814, %r813;
	mov.u32 	%r820, %r818;

BB0_285:
	shl.b32 	%r742, %r166, 1;
	sub.s32 	%r741, %r227, %r734;
	setp.lt.u32	%p268, %r741, 4;
	@%p268 bra 	BB0_287;

BB0_286:
	mul.lo.s32 	%r594, %r819, %r818;
	add.s32 	%r595, %r819, -1;
	mul.lo.s32 	%r596, %r594, %r595;
	add.s32 	%r597, %r819, -2;
	mul.lo.s32 	%r598, %r596, %r597;
	add.s32 	%r599, %r819, -3;
	mul.lo.s32 	%r818, %r598, %r599;
	add.s32 	%r819, %r819, -4;
	setp.gt.s32	%p269, %r819, 0;
	mov.u32 	%r820, %r818;
	@%p269 bra 	BB0_286;

BB0_287:
	cvt.rn.f64.s32	%fd482, %r809;
	mul.lo.s32 	%r600, %r820, %r812;
	cvt.rn.f64.s32	%fd483, %r600;
	div.rn.f64 	%fd200, %fd482, %fd483;
	cvt.rn.f64.s32	%fd201, %r166;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r204}, %fd201;
	}
	bfe.u32 	%r601, %r204, 20, 11;
	add.s32 	%r602, %r601, -1012;
	mov.b64 	 %rd190, %fd201;
	shl.b64 	%rd191, %rd190, %r602;
	setp.eq.s64	%p270, %rd191, -9223372036854775808;
	mov.f64 	%fd484, 0d3FF0000000000000;
	// Callseq Start 16
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd484;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd201;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd206, [retval0+0];
	
	//{
	}// Callseq End 16
	setp.lt.s32	%p271, %r80, 0;
	and.pred  	%p11, %p271, %p270;
	@!%p11 bra 	BB0_289;
	bra.uni 	BB0_288;

BB0_288:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r603}, %fd206;
	}
	xor.b32  	%r604, %r603, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r605, %temp}, %fd206;
	}
	mov.b64 	%fd206, {%r605, %r604};

BB0_289:
	setp.gt.s32	%p272, %r80, -1;
	@%p272 bra 	BB0_291;

	cvt.rzi.f64.f64	%fd485, %fd201;
	setp.neu.f64	%p273, %fd485, %fd201;
	selp.f64	%fd206, 0dFFF8000000000000, %fd206, %p273;

BB0_291:
	add.f64 	%fd207, %fd201, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r205}, %fd207;
	}
	and.b32  	%r206, %r205, 2146435072;
	setp.ne.s32	%p274, %r206, 2146435072;
	@%p274 bra 	BB0_292;

	abs.f64 	%fd486, %fd201;
	setp.gtu.f64	%p275, %fd486, 0d7FF0000000000000;
	mov.f64 	%fd679, %fd207;
	@%p275 bra 	BB0_301;

	cvt.rn.f64.s32	%fd619, %r166;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r743}, %fd619;
	}
	and.b32  	%r606, %r743, 2147483647;
	setp.ne.s32	%p276, %r606, 2146435072;
	@%p276 bra 	BB0_296;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r607, %temp}, %fd201;
	}
	setp.eq.s32	%p277, %r607, 0;
	@%p277 bra 	BB0_300;
	bra.uni 	BB0_296;

BB0_300:
	mov.u32 	%r615, 1072693248;
	mov.u32 	%r616, 0;
	mov.b64 	%fd679, {%r616, %r615};
	bra.uni 	BB0_301;

BB0_292:
	mov.f64 	%fd679, %fd206;

BB0_301:
	shr.u32 	%r617, %r206, 20;
	add.s32 	%r618, %r617, -1012;
	mov.b64 	 %rd192, %fd207;
	shl.b64 	%rd37, %rd192, %r618;
	setp.eq.s64	%p280, %rd37, -9223372036854775808;
	// Callseq Start 17
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd196;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd207;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd681, [retval0+0];
	
	//{
	}// Callseq End 17
	setp.lt.s32	%p281, %r154, 0;
	and.pred  	%p12, %p281, %p280;
	@!%p12 bra 	BB0_303;
	bra.uni 	BB0_302;

BB0_302:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r619}, %fd681;
	}
	xor.b32  	%r620, %r619, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r621, %temp}, %fd681;
	}
	mov.b64 	%fd681, {%r621, %r620};

BB0_303:
	setp.eq.f32	%p282, %f30, 0f00000000;
	@%p282 bra 	BB0_306;
	bra.uni 	BB0_304;

BB0_306:
	selp.b32	%r622, %r154, 0, %p280;
	or.b32  	%r623, %r622, 2146435072;
	setp.lt.s32	%p286, %r205, 0;
	selp.b32	%r624, %r623, %r622, %p286;
	mov.u32 	%r625, 0;
	mov.b64 	%fd681, {%r625, %r624};
	bra.uni 	BB0_307;

BB0_304:
	setp.gt.s32	%p283, %r154, -1;
	@%p283 bra 	BB0_307;

	cvt.rzi.f64.f64	%fd488, %fd207;
	setp.neu.f64	%p284, %fd488, %fd207;
	selp.f64	%fd681, 0dFFF8000000000000, %fd681, %p284;

BB0_307:
	add.f64 	%fd217, %fd195, %fd207;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r626}, %fd217;
	}
	and.b32  	%r627, %r626, 2146435072;
	setp.ne.s32	%p287, %r627, 2146435072;
	setp.gtu.f64	%p288, %fd196, 0d7FF0000000000000;
	or.pred  	%p289, %p287, %p288;
	selp.f64	%fd682, %fd681, %fd217, %p287;
	@%p289 bra 	BB0_316;

	abs.f64 	%fd489, %fd207;
	setp.gtu.f64	%p290, %fd489, 0d7FF0000000000000;
	mov.f64 	%fd682, %fd217;
	@%p290 bra 	BB0_316;

	and.b32  	%r628, %r205, 2147483647;
	setp.ne.s32	%p291, %r628, 2146435072;
	@%p291 bra 	BB0_311;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r629, %temp}, %fd207;
	}
	setp.eq.s32	%p292, %r629, 0;
	@%p292 bra 	BB0_315;
	bra.uni 	BB0_311;

BB0_315:
	setp.eq.f32	%p295, %f30, 0fBF800000;
	setp.lt.s32	%p296, %r205, 0;
	selp.b32	%r637, %r156, %r155, %p296;
	selp.b32	%r638, 1072693248, %r637, %p295;
	mov.u32 	%r639, 0;
	mov.b64 	%fd682, {%r639, %r638};
	bra.uni 	BB0_316;

BB0_296:
	setp.ne.s32	%p278, %r81, 2146435072;
	@%p278 bra 	BB0_297;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r608, %temp}, %fd449;
	}
	setp.ne.s32	%p279, %r608, 0;
	mov.f64 	%fd679, %fd206;
	@%p279 bra 	BB0_301;

	cvt.rn.f64.s32	%fd620, %r166;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r744}, %fd620;
	}
	shr.s32 	%r609, %r744, 31;
	and.b32  	%r610, %r609, -2146435072;
	add.s32 	%r611, %r610, 2146435072;
	or.b32  	%r612, %r611, -2147483648;
	selp.b32	%r613, %r612, %r611, %p11;
	mov.u32 	%r614, 0;
	mov.b64 	%fd679, {%r614, %r613};
	bra.uni 	BB0_301;

BB0_311:
	setp.ne.s32	%p293, %r157, 2146435072;
	@%p293 bra 	BB0_312;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r630, %temp}, %fd195;
	}
	setp.ne.s32	%p294, %r630, 0;
	mov.f64 	%fd682, %fd681;
	@%p294 bra 	BB0_316;

	shr.s32 	%r631, %r205, 31;
	and.b32  	%r632, %r631, -2146435072;
	add.s32 	%r633, %r632, 2146435072;
	or.b32  	%r634, %r633, -2147483648;
	selp.b32	%r635, %r634, %r633, %p12;
	mov.u32 	%r636, 0;
	mov.b64 	%fd682, {%r636, %r635};
	bra.uni 	BB0_316;

BB0_297:
	mov.f64 	%fd679, %fd206;
	bra.uni 	BB0_301;

BB0_312:
	mov.f64 	%fd682, %fd681;

BB0_316:
	shl.b32 	%r706, %r166, 1;
	sub.s32 	%r705, %r227, %r706;
	setp.lt.s32	%p355, %r102, 0;
	setp.eq.s32	%p297, %r166, 0;
	selp.f64	%fd490, 0d3FF0000000000000, %fd679, %p297;
	setp.eq.f64	%p298, %fd207, 0d0000000000000000;
	setp.eq.f32	%p299, %f30, 0f3F800000;
	or.pred  	%p300, %p299, %p298;
	selp.f64	%fd491, 0d3FF0000000000000, %fd682, %p300;
	mul.f64 	%fd222, %fd490, %fd491;
	cvt.rn.f64.s32	%fd492, %r705;
	add.f64 	%fd223, %fd492, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r207}, %fd223;
	}
	bfe.u32 	%r640, %r207, 20, 11;
	add.s32 	%r641, %r640, -1012;
	mov.b64 	 %rd193, %fd223;
	shl.b64 	%rd38, %rd193, %r641;
	setp.eq.s64	%p301, %rd38, -9223372036854775808;
	// Callseq Start 18
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd122;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd223;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd684, [retval0+0];
	
	//{
	}// Callseq End 18
	and.pred  	%p13, %p355, %p301;
	@!%p13 bra 	BB0_318;
	bra.uni 	BB0_317;

BB0_317:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r642}, %fd684;
	}
	xor.b32  	%r643, %r642, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r644, %temp}, %fd684;
	}
	mov.b64 	%fd684, {%r644, %r643};

BB0_318:
	setp.eq.f32	%p356, %f29, 0f00000000;
	@%p356 bra 	BB0_321;
	bra.uni 	BB0_319;

BB0_321:
	selp.b32	%r645, %r102, 0, %p301;
	or.b32  	%r646, %r645, 2146435072;
	setp.lt.s32	%p307, %r207, 0;
	selp.b32	%r647, %r646, %r645, %p307;
	mov.u32 	%r648, 0;
	mov.b64 	%fd684, {%r648, %r647};
	bra.uni 	BB0_322;

BB0_319:
	setp.gt.s32	%p304, %r102, -1;
	@%p304 bra 	BB0_322;

	cvt.rzi.f64.f64	%fd493, %fd223;
	setp.neu.f64	%p305, %fd493, %fd223;
	selp.f64	%fd684, 0dFFF8000000000000, %fd684, %p305;

BB0_322:
	setp.gtu.f64	%p357, %fd122, 0d7FF0000000000000;
	add.f64 	%fd230, %fd121, %fd223;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r649}, %fd230;
	}
	and.b32  	%r650, %r649, 2146435072;
	setp.ne.s32	%p308, %r650, 2146435072;
	or.pred  	%p310, %p308, %p357;
	selp.f64	%fd685, %fd684, %fd230, %p308;
	@%p310 bra 	BB0_331;

	abs.f64 	%fd494, %fd223;
	setp.gtu.f64	%p311, %fd494, 0d7FF0000000000000;
	mov.f64 	%fd685, %fd230;
	@%p311 bra 	BB0_331;

	and.b32  	%r651, %r207, 2147483647;
	setp.ne.s32	%p312, %r651, 2146435072;
	@%p312 bra 	BB0_326;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r652, %temp}, %fd223;
	}
	setp.eq.s32	%p313, %r652, 0;
	@%p313 bra 	BB0_330;
	bra.uni 	BB0_326;

BB0_330:
	setp.eq.f32	%p359, %f29, 0fBF800000;
	selp.b32	%r710, 2146435072, 0, %p247;
	setp.gt.f64	%p358, %fd122, 0d3FF0000000000000;
	selp.b32	%r709, 2146435072, 0, %p358;
	xor.b32  	%r708, %r709, 2146435072;
	setp.lt.s32	%p317, %r207, 0;
	selp.b32	%r660, %r708, %r709, %p317;
	selp.b32	%r661, 1072693248, %r660, %p359;
	mov.u32 	%r662, 0;
	mov.b64 	%fd685, {%r662, %r661};
	bra.uni 	BB0_331;

BB0_326:
	and.b32  	%r707, %r102, 2147483647;
	setp.ne.s32	%p314, %r707, 2146435072;
	@%p314 bra 	BB0_327;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r653, %temp}, %fd121;
	}
	setp.ne.s32	%p315, %r653, 0;
	mov.f64 	%fd685, %fd684;
	@%p315 bra 	BB0_331;

	setp.lt.s32	%p365, %r102, 0;
	and.pred  	%p364, %p365, %p301;
	shr.s32 	%r654, %r207, 31;
	and.b32  	%r655, %r654, -2146435072;
	add.s32 	%r656, %r655, 2146435072;
	or.b32  	%r657, %r656, -2147483648;
	selp.b32	%r658, %r657, %r656, %p364;
	mov.u32 	%r659, 0;
	mov.b64 	%fd685, {%r659, %r658};
	bra.uni 	BB0_331;

BB0_327:
	mov.f64 	%fd685, %fd684;

BB0_331:
	setp.eq.f32	%p360, %f29, 0f3F800000;
	setp.eq.f64	%p318, %fd223, 0d0000000000000000;
	or.pred  	%p320, %p360, %p318;
	selp.f64	%fd495, 0d3FF0000000000000, %fd685, %p320;
	mul.f64 	%fd235, %fd222, %fd495;
	mov.f64 	%fd687, %fd197;
	@!%p9 bra 	BB0_333;
	bra.uni 	BB0_332;

BB0_332:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r663}, %fd197;
	}
	xor.b32  	%r664, %r663, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r665, %temp}, %fd197;
	}
	mov.b64 	%fd687, {%r665, %r664};

BB0_333:
	setp.eq.f32	%p361, %f29, 0f00000000;
	@%p361 bra 	BB0_336;
	bra.uni 	BB0_334;

BB0_336:
	mov.u32 	%r666, 0;
	mov.b64 	%fd687, {%r666, %r161};
	bra.uni 	BB0_337;

BB0_334:
	setp.gt.s32	%p322, %r102, -1;
	@%p322 bra 	BB0_337;

	mov.f64 	%fd618, 0d4000000000000000;
	cvt.rzi.f64.f64	%fd497, %fd618;
	setp.neu.f64	%p323, %fd497, 0d4000000000000000;
	selp.f64	%fd687, 0dFFF8000000000000, %fd687, %p323;

BB0_337:
	cvt.f64.f32	%fd613, %f29;
	add.f64 	%fd612, %fd613, 0d4000000000000000;
	selp.f64	%fd688, %fd687, %fd612, %p251;
	@%p10 bra 	BB0_345;

	mov.f64 	%fd614, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r712}, %fd614;
	}
	and.b32  	%r711, %r712, 2147483647;
	setp.ne.s32	%p325, %r711, 2146435072;
	@%p325 bra 	BB0_340;

	mov.f64 	%fd616, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r667, %temp}, %fd616;
	}
	setp.eq.s32	%p326, %r667, 0;
	@%p326 bra 	BB0_344;
	bra.uni 	BB0_340;

BB0_344:
	mov.u32 	%r670, 0;
	mov.b64 	%fd688, {%r670, %r164};
	bra.uni 	BB0_345;

BB0_340:
	and.b32  	%r713, %r102, 2147483647;
	setp.ne.s32	%p327, %r713, 2146435072;
	@%p327 bra 	BB0_341;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r668, %temp}, %fd121;
	}
	setp.ne.s32	%p328, %r668, 0;
	mov.f64 	%fd688, %fd687;
	@%p328 bra 	BB0_345;

	mov.f64 	%fd615, 0d4000000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r719}, %fd615;
	}
	shr.s32 	%r718, %r719, 31;
	and.b32  	%r717, %r718, -2146435072;
	add.s32 	%r716, %r717, 2146435072;
	or.b32  	%r715, %r716, -2147483648;
	selp.b32	%r714, %r715, %r716, %p9;
	mov.u32 	%r669, 0;
	mov.b64 	%fd688, {%r669, %r714};
	bra.uni 	BB0_345;

BB0_341:
	mov.f64 	%fd688, %fd687;

BB0_345:
	cvt.rn.f32.f64	%f258, %fd200;
	setp.eq.f32	%p362, %f29, 0f3F800000;
	mad.lo.s32 	%r671, %r166, -2, %r227;
	cvt.rn.f64.s32	%fd499, %r671;
	selp.f64	%fd500, 0d3FF0000000000000, %fd688, %p362;
	mul.f64 	%fd501, %fd119, %fd500;
	sub.f64 	%fd502, %fd499, %fd501;
	mul.f64 	%fd503, %fd235, %fd502;
	cvt.f64.f32	%fd504, %f258;
	fma.rn.f64 	%fd505, %fd504, %fd503, %fd689;
	add.s32 	%r801, %r166, 1;
	cvt.rn.f32.f64	%f126, %fd505;
	cvt.f64.f32	%fd689, %f126;
	setp.lt.s32	%p330, %r166, %r72;
	@%p330 bra 	BB0_269;

BB0_346:
	cvt.rn.f64.s32	%fd247, %r780;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r209}, %fd247;
	}
	bfe.u32 	%r672, %r209, 20, 11;
	add.s32 	%r673, %r672, -1012;
	mov.b64 	 %rd194, %fd247;
	shl.b64 	%rd195, %rd194, %r673;
	setp.eq.s64	%p331, %rd195, -9223372036854775808;
	mov.f64 	%fd506, 0d3FF0000000000000;
	// Callseq Start 19
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd506;
	.param .b64 param1;
	st.param.f64	[param1+0], %fd247;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_accurate_pow, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd252, [retval0+0];
	
	//{
	}// Callseq End 19
	setp.lt.s32	%p332, %r80, 0;
	and.pred  	%p14, %p332, %p331;
	@!%p14 bra 	BB0_348;
	bra.uni 	BB0_347;

BB0_347:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r674}, %fd252;
	}
	xor.b32  	%r675, %r674, -2147483648;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r676, %temp}, %fd252;
	}
	mov.b64 	%fd252, {%r676, %r675};

BB0_348:
	setp.gt.s32	%p333, %r80, -1;
	@%p333 bra 	BB0_350;

	cvt.rzi.f64.f64	%fd507, %fd247;
	setp.neu.f64	%p334, %fd507, %fd247;
	selp.f64	%fd252, 0dFFF8000000000000, %fd252, %p334;

BB0_350:
	add.f64 	%fd692, %fd247, 0dBFF0000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r677}, %fd692;
	}
	and.b32  	%r678, %r677, 2146435072;
	setp.ne.s32	%p335, %r678, 2146435072;
	@%p335 bra 	BB0_351;

	abs.f64 	%fd508, %fd247;
	setp.gtu.f64	%p336, %fd508, 0d7FF0000000000000;
	@%p336 bra 	BB0_360;

	and.b32  	%r679, %r209, 2147483647;
	setp.ne.s32	%p337, %r679, 2146435072;
	@%p337 bra 	BB0_355;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r680, %temp}, %fd247;
	}
	setp.eq.s32	%p338, %r680, 0;
	@%p338 bra 	BB0_359;
	bra.uni 	BB0_355;

BB0_359:
	mov.u32 	%r688, 1072693248;
	mov.u32 	%r689, 0;
	mov.b64 	%fd692, {%r689, %r688};
	bra.uni 	BB0_360;

BB0_351:
	mov.f64 	%fd692, %fd252;

BB0_360:
	ld.param.u64 	%rd275, [_Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_1];
	rem.s32 	%r720, %r773, %r226;
	setp.eq.s32	%p341, %r780, 0;
	selp.f64	%fd510, 0d3FF0000000000000, %fd692, %p341;
	mul.f64 	%fd511, %fd689, %fd510;
	cvt.rn.f32.f64	%f35, %fd511;
	mul.wide.s32 	%rd197, %r773, 4;
	add.s64 	%rd196, %rd275, %rd197;
	// inline asm
	ld.global.nc.f32 %f127, [%rd196];
	// inline asm
	mul.lo.s32 	%r210, %r720, %r225;
	setp.lt.s32	%p342, %r225, 1;
	@%p342 bra 	BB0_370;

	mov.u32 	%r821, 0;
	setp.eq.s32	%p343, %r82, 0;
	@%p343 bra 	BB0_367;

	setp.eq.s32	%p344, %r82, 1;
	@%p344 bra 	BB0_366;

	setp.eq.s32	%p345, %r82, 2;
	@%p345 bra 	BB0_365;

	mul.wide.s32 	%rd204, %r210, 4;
	add.s64 	%rd205, %rd2, %rd204;
	// inline asm
	ld.global.nc.f32 %f128, [%rd65];
	// inline asm
	mul.f32 	%f134, %f127, %f128;
	cvt.f64.f32	%fd512, %f134;
	// inline asm
	ld.global.nc.f32 %f129, [%rd65];
	// inline asm
	cvt.f64.f32	%fd513, %f129;
	add.f64 	%fd514, %fd513, 0d3FF0000000000000;
	div.rn.f64 	%fd515, %fd512, %fd514;
	mul.wide.s32 	%rd206, %r87, 4;
	add.s64 	%rd200, %rd63, %rd206;
	// inline asm
	ld.global.nc.f32 %f130, [%rd200];
	// inline asm
	cvt.f64.f32	%fd516, %f130;
	mul.f64 	%fd517, %fd515, %fd516;
	cvt.rn.f32.f64	%f135, %fd517;
	atom.global.add.f32 	%f136, [%rd205], %f135;
	// inline asm
	ld.global.nc.f32 %f131, [%rd65];
	// inline asm
	mul.f32 	%f137, %f127, %f131;
	cvt.f64.f32	%fd518, %f137;
	// inline asm
	ld.global.nc.f32 %f132, [%rd65];
	// inline asm
	cvt.f64.f32	%fd519, %f132;
	add.f64 	%fd520, %fd519, 0d3FF0000000000000;
	div.rn.f64 	%fd521, %fd518, %fd520;
	add.s64 	%rd203, %rd64, %rd204;
	// inline asm
	ld.global.nc.f32 %f133, [%rd203];
	// inline asm
	cvt.f64.f32	%fd522, %f133;
	mul.f64 	%fd523, %fd521, %fd522;
	cvt.rn.f32.f64	%f138, %fd523;
	add.s64 	%rd207, %rd1, %rd206;
	atom.global.add.f32 	%f139, [%rd207], %f138;
	mov.u32 	%r821, 1;

BB0_365:
	add.s32 	%r694, %r821, %r210;
	mul.wide.s32 	%rd214, %r694, 4;
	add.s64 	%rd215, %rd2, %rd214;
	// inline asm
	ld.global.nc.f32 %f140, [%rd65];
	// inline asm
	mul.f32 	%f146, %f127, %f140;
	cvt.f64.f32	%fd524, %f146;
	// inline asm
	ld.global.nc.f32 %f141, [%rd65];
	// inline asm
	cvt.f64.f32	%fd525, %f141;
	add.f64 	%fd526, %fd525, 0d3FF0000000000000;
	div.rn.f64 	%fd527, %fd524, %fd526;
	add.s32 	%r695, %r821, %r87;
	mul.wide.s32 	%rd216, %r695, 4;
	add.s64 	%rd210, %rd63, %rd216;
	// inline asm
	ld.global.nc.f32 %f142, [%rd210];
	// inline asm
	cvt.f64.f32	%fd528, %f142;
	mul.f64 	%fd529, %fd527, %fd528;
	cvt.rn.f32.f64	%f147, %fd529;
	atom.global.add.f32 	%f148, [%rd215], %f147;
	add.s64 	%rd217, %rd1, %rd216;
	// inline asm
	ld.global.nc.f32 %f143, [%rd65];
	// inline asm
	mul.f32 	%f149, %f127, %f143;
	cvt.f64.f32	%fd530, %f149;
	// inline asm
	ld.global.nc.f32 %f144, [%rd65];
	// inline asm
	cvt.f64.f32	%fd531, %f144;
	add.f64 	%fd532, %fd531, 0d3FF0000000000000;
	div.rn.f64 	%fd533, %fd530, %fd532;
	add.s64 	%rd213, %rd64, %rd214;
	// inline asm
	ld.global.nc.f32 %f145, [%rd213];
	// inline asm
	cvt.f64.f32	%fd534, %f145;
	mul.f64 	%fd535, %fd533, %fd534;
	cvt.rn.f32.f64	%f150, %fd535;
	atom.global.add.f32 	%f151, [%rd217], %f150;
	add.s32 	%r821, %r821, 1;

BB0_366:
	add.s32 	%r696, %r821, %r210;
	mul.wide.s32 	%rd224, %r696, 4;
	add.s64 	%rd225, %rd2, %rd224;
	// inline asm
	ld.global.nc.f32 %f152, [%rd65];
	// inline asm
	mul.f32 	%f158, %f127, %f152;
	cvt.f64.f32	%fd536, %f158;
	// inline asm
	ld.global.nc.f32 %f153, [%rd65];
	// inline asm
	cvt.f64.f32	%fd537, %f153;
	add.f64 	%fd538, %fd537, 0d3FF0000000000000;
	div.rn.f64 	%fd539, %fd536, %fd538;
	add.s32 	%r697, %r821, %r87;
	mul.wide.s32 	%rd226, %r697, 4;
	add.s64 	%rd220, %rd63, %rd226;
	// inline asm
	ld.global.nc.f32 %f154, [%rd220];
	// inline asm
	cvt.f64.f32	%fd540, %f154;
	mul.f64 	%fd541, %fd539, %fd540;
	cvt.rn.f32.f64	%f159, %fd541;
	atom.global.add.f32 	%f160, [%rd225], %f159;
	add.s64 	%rd227, %rd1, %rd226;
	// inline asm
	ld.global.nc.f32 %f155, [%rd65];
	// inline asm
	mul.f32 	%f161, %f127, %f155;
	cvt.f64.f32	%fd542, %f161;
	// inline asm
	ld.global.nc.f32 %f156, [%rd65];
	// inline asm
	cvt.f64.f32	%fd543, %f156;
	add.f64 	%fd544, %fd543, 0d3FF0000000000000;
	div.rn.f64 	%fd545, %fd542, %fd544;
	add.s64 	%rd223, %rd64, %rd224;
	// inline asm
	ld.global.nc.f32 %f157, [%rd223];
	// inline asm
	cvt.f64.f32	%fd546, %f157;
	mul.f64 	%fd547, %fd545, %fd546;
	cvt.rn.f32.f64	%f162, %fd547;
	atom.global.add.f32 	%f163, [%rd227], %f162;
	add.s32 	%r821, %r821, 1;

BB0_367:
	setp.lt.u32	%p346, %r225, 4;
	@%p346 bra 	BB0_370;

	div.s32 	%r730, %r773, %r226;
	rem.s32 	%r729, %r773, %r226;
	mul.wide.s32 	%rd286, %r821, 4;
	mul.lo.s32 	%r698, %r225, %r730;
	mul.wide.s32 	%rd228, %r698, 4;
	add.s64 	%rd40, %rd24, %rd228;
	add.s64 	%rd41, %rd23, %rd228;
	mul.lo.s32 	%r699, %r225, %r729;
	mul.wide.s32 	%rd229, %r699, 4;
	add.s64 	%rd42, %rd22, %rd229;
	add.s64 	%rd43, %rd25, %rd229;

BB0_369:
	// inline asm
	ld.global.nc.f32 %f164, [%rd65];
	// inline asm
	mul.f32 	%f188, %f127, %f164;
	cvt.f64.f32	%fd548, %f188;
	// inline asm
	ld.global.nc.f32 %f165, [%rd65];
	// inline asm
	cvt.f64.f32	%fd549, %f165;
	add.f64 	%fd550, %fd549, 0d3FF0000000000000;
	div.rn.f64 	%fd551, %fd548, %fd550;
	add.s64 	%rd250, %rd41, %rd286;
	add.s64 	%rd232, %rd250, -12;
	// inline asm
	ld.global.nc.f32 %f166, [%rd232];
	// inline asm
	cvt.f64.f32	%fd552, %f166;
	mul.f64 	%fd553, %fd551, %fd552;
	cvt.rn.f32.f64	%f189, %fd553;
	add.s64 	%rd254, %rd43, %rd286;
	add.s64 	%rd255, %rd254, -12;
	atom.global.add.f32 	%f190, [%rd255], %f189;
	// inline asm
	ld.global.nc.f32 %f167, [%rd65];
	// inline asm
	mul.f32 	%f191, %f127, %f167;
	cvt.f64.f32	%fd554, %f191;
	// inline asm
	ld.global.nc.f32 %f168, [%rd65];
	// inline asm
	cvt.f64.f32	%fd555, %f168;
	add.f64 	%fd556, %fd555, 0d3FF0000000000000;
	div.rn.f64 	%fd557, %fd554, %fd556;
	add.s64 	%rd253, %rd42, %rd286;
	add.s64 	%rd235, %rd253, -12;
	// inline asm
	ld.global.nc.f32 %f169, [%rd235];
	// inline asm
	cvt.f64.f32	%fd558, %f169;
	mul.f64 	%fd559, %fd557, %fd558;
	cvt.rn.f32.f64	%f192, %fd559;
	add.s64 	%rd256, %rd40, %rd286;
	add.s64 	%rd257, %rd256, -12;
	atom.global.add.f32 	%f193, [%rd257], %f192;
	// inline asm
	ld.global.nc.f32 %f170, [%rd65];
	// inline asm
	mul.f32 	%f194, %f127, %f170;
	cvt.f64.f32	%fd560, %f194;
	// inline asm
	ld.global.nc.f32 %f171, [%rd65];
	// inline asm
	cvt.f64.f32	%fd561, %f171;
	add.f64 	%fd562, %fd561, 0d3FF0000000000000;
	div.rn.f64 	%fd563, %fd560, %fd562;
	add.s64 	%rd238, %rd250, -8;
	// inline asm
	ld.global.nc.f32 %f172, [%rd238];
	// inline asm
	cvt.f64.f32	%fd564, %f172;
	mul.f64 	%fd565, %fd563, %fd564;
	cvt.rn.f32.f64	%f195, %fd565;
	add.s64 	%rd258, %rd254, -8;
	atom.global.add.f32 	%f196, [%rd258], %f195;
	// inline asm
	ld.global.nc.f32 %f173, [%rd65];
	// inline asm
	mul.f32 	%f197, %f127, %f173;
	cvt.f64.f32	%fd566, %f197;
	// inline asm
	ld.global.nc.f32 %f174, [%rd65];
	// inline asm
	cvt.f64.f32	%fd567, %f174;
	add.f64 	%fd568, %fd567, 0d3FF0000000000000;
	div.rn.f64 	%fd569, %fd566, %fd568;
	add.s64 	%rd241, %rd253, -8;
	// inline asm
	ld.global.nc.f32 %f175, [%rd241];
	// inline asm
	cvt.f64.f32	%fd570, %f175;
	mul.f64 	%fd571, %fd569, %fd570;
	cvt.rn.f32.f64	%f198, %fd571;
	add.s64 	%rd259, %rd256, -8;
	atom.global.add.f32 	%f199, [%rd259], %f198;
	// inline asm
	ld.global.nc.f32 %f176, [%rd65];
	// inline asm
	mul.f32 	%f200, %f127, %f176;
	cvt.f64.f32	%fd572, %f200;
	// inline asm
	ld.global.nc.f32 %f177, [%rd65];
	// inline asm
	cvt.f64.f32	%fd573, %f177;
	add.f64 	%fd574, %fd573, 0d3FF0000000000000;
	div.rn.f64 	%fd575, %fd572, %fd574;
	add.s64 	%rd244, %rd250, -4;
	// inline asm
	ld.global.nc.f32 %f178, [%rd244];
	// inline asm
	cvt.f64.f32	%fd576, %f178;
	mul.f64 	%fd577, %fd575, %fd576;
	cvt.rn.f32.f64	%f201, %fd577;
	add.s64 	%rd260, %rd254, -4;
	atom.global.add.f32 	%f202, [%rd260], %f201;
	// inline asm
	ld.global.nc.f32 %f179, [%rd65];
	// inline asm
	mul.f32 	%f203, %f127, %f179;
	cvt.f64.f32	%fd578, %f203;
	// inline asm
	ld.global.nc.f32 %f180, [%rd65];
	// inline asm
	cvt.f64.f32	%fd579, %f180;
	add.f64 	%fd580, %fd579, 0d3FF0000000000000;
	div.rn.f64 	%fd581, %fd578, %fd580;
	add.s64 	%rd247, %rd253, -4;
	// inline asm
	ld.global.nc.f32 %f181, [%rd247];
	// inline asm
	cvt.f64.f32	%fd582, %f181;
	mul.f64 	%fd583, %fd581, %fd582;
	cvt.rn.f32.f64	%f204, %fd583;
	add.s64 	%rd261, %rd256, -4;
	atom.global.add.f32 	%f205, [%rd261], %f204;
	// inline asm
	ld.global.nc.f32 %f182, [%rd65];
	// inline asm
	mul.f32 	%f206, %f127, %f182;
	cvt.f64.f32	%fd584, %f206;
	// inline asm
	ld.global.nc.f32 %f183, [%rd65];
	// inline asm
	cvt.f64.f32	%fd585, %f183;
	add.f64 	%fd586, %fd585, 0d3FF0000000000000;
	div.rn.f64 	%fd587, %fd584, %fd586;
	// inline asm
	ld.global.nc.f32 %f184, [%rd250];
	// inline asm
	cvt.f64.f32	%fd588, %f184;
	mul.f64 	%fd589, %fd587, %fd588;
	cvt.rn.f32.f64	%f207, %fd589;
	atom.global.add.f32 	%f208, [%rd254], %f207;
	// inline asm
	ld.global.nc.f32 %f185, [%rd65];
	// inline asm
	mul.f32 	%f209, %f127, %f185;
	cvt.f64.f32	%fd590, %f209;
	// inline asm
	ld.global.nc.f32 %f186, [%rd65];
	// inline asm
	cvt.f64.f32	%fd591, %f186;
	add.f64 	%fd592, %fd591, 0d3FF0000000000000;
	div.rn.f64 	%fd593, %fd590, %fd592;
	// inline asm
	ld.global.nc.f32 %f187, [%rd253];
	// inline asm
	cvt.f64.f32	%fd594, %f187;
	mul.f64 	%fd595, %fd593, %fd594;
	cvt.rn.f32.f64	%f210, %fd595;
	atom.global.add.f32 	%f211, [%rd256], %f210;
	add.s64 	%rd286, %rd286, 16;
	add.s32 	%r821, %r821, 4;
	setp.lt.s32	%p347, %r821, %r225;
	@%p347 bra 	BB0_369;

BB0_370:
	div.s32 	%r722, %r773, %r226;
	mul.wide.s32 	%rd278, %r722, 4;
	ld.param.u64 	%rd277, [_Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_5];
	add.s64 	%rd276, %rd277, %rd278;
	rem.s32 	%r721, %r773, %r226;
	setp.gt.s32	%p348, %r225, 0;
	// inline asm
	ld.global.nc.s32 %r700, [%rd276];
	// inline asm
	setp.eq.s32	%p349, %r700, %r721;
	and.pred  	%p350, %p349, %p348;
	@!%p350 bra 	BB0_379;
	bra.uni 	BB0_371;

BB0_371:
	div.s32 	%r728, %r773, %r226;
	cvt.s64.s32	%rd281, %r728;
	rem.s32 	%r727, %r773, %r226;
	cvt.u32.u64	%r702, %rd281;
	mul.wide.s32 	%rd263, %r727, 4;
	add.s64 	%rd46, %rd5, %rd263;
	mul.f32 	%f212, %f19, %f127;
	cvt.f64.f32	%fd257, %f212;
	mul.lo.s32 	%r703, %r225, %r727;
	mul.wide.s32 	%rd264, %r703, 4;
	add.s64 	%rd291, %rd64, %rd264;
	add.s64 	%rd290, %rd2, %rd264;
	mul.lo.s32 	%r704, %r225, %r702;
	mul.wide.s32 	%rd265, %r704, 4;
	add.s64 	%rd289, %rd63, %rd265;
	add.s64 	%rd288, %rd1, %rd265;
	add.s64 	%rd287, %rd19, %rd265;
	mov.u32 	%r825, 0;

BB0_372:
	ld.param.s8 	%rs5, [_Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_10];
	ld.global.f32 	%f37, [%rd46];
	mul.f32 	%f38, %f19, %f37;
	and.b16  	%rs3, %rs5, 255;
	setp.eq.s16	%p351, %rs3, 0;
	@%p351 bra 	BB0_374;
	bra.uni 	BB0_373;

BB0_374:
	mul.f32 	%f217, %f37, %f38;
	div.rn.f32 	%f218, %f35, %f217;
	// inline asm
	ld.global.nc.f32 %f215, [%rd289];
	// inline asm
	ld.global.f32 	%f219, [%rd46];
	mul.f32 	%f220, %f215, %f219;
	// inline asm
	ld.global.nc.f32 %f216, [%rd291];
	// inline asm
	mul.f32 	%f221, %f29, %f38;
	mul.f32 	%f222, %f221, %f216;
	ld.global.f32 	%f223, [%rd46];
	div.rn.f32 	%f224, %f222, %f223;
	sub.f32 	%f225, %f220, %f224;
	mul.f32 	%f276, %f218, %f225;
	bra.uni 	BB0_375;

BB0_373:
	ld.global.f32 	%f213, [%rd287];
	mul.f32 	%f214, %f35, %f213;
	div.rn.f32 	%f276, %f214, %f19;

BB0_375:
	@%p351 bra 	BB0_377;
	bra.uni 	BB0_376;

BB0_377:
	// inline asm
	ld.global.nc.f32 %f229, [%rd65];
	// inline asm
	cvt.f64.f32	%fd601, %f229;
	add.f64 	%fd602, %fd601, 0d3FF0000000000000;
	div.rn.f64 	%fd603, %fd257, %fd602;
	// inline asm
	ld.global.nc.f32 %f230, [%rd291];
	// inline asm
	mul.f32 	%f231, %f275, %f230;
	ld.global.f32 	%f232, [%rd46];
	div.rn.f32 	%f233, %f231, %f232;
	fma.rn.f32 	%f234, %f276, %f232, %f233;
	cvt.f64.f32	%fd604, %f234;
	mul.f64 	%fd605, %fd603, %fd604;
	cvt.rn.f32.f64	%f235, %fd605;
	atom.global.add.f32 	%f236, [%rd290], %f235;
	bra.uni 	BB0_378;

BB0_376:
	// inline asm
	ld.global.nc.f32 %f226, [%rd65];
	// inline asm
	cvt.f64.f32	%fd596, %f226;
	add.f64 	%fd597, %fd596, 0d3FF0000000000000;
	div.rn.f64 	%fd598, %fd257, %fd597;
	cvt.f64.f32	%fd599, %f276;
	mul.f64 	%fd600, %fd599, %fd598;
	cvt.rn.f32.f64	%f227, %fd600;
	atom.global.add.f32 	%f228, [%rd290], %f227;

BB0_378:
	ld.global.f32 	%f241, [%rd46];
	mul.f32 	%f242, %f19, %f241;
	mul.f32 	%f243, %f19, %f242;
	div.rn.f32 	%f244, %f35, %f243;
	// inline asm
	ld.global.nc.f32 %f237, [%rd291];
	// inline asm
	mul.f32 	%f245, %f19, %f237;
	// inline asm
	ld.global.nc.f32 %f238, [%rd289];
	// inline asm
	mul.f32 	%f246, %f29, %f38;
	mul.f32 	%f247, %f246, %f238;
	div.rn.f32 	%f248, %f247, %f19;
	sub.f32 	%f249, %f245, %f248;
	mul.f32 	%f250, %f244, %f249;
	ld.global.f32 	%f251, [%rd46];
	mul.f32 	%f252, %f127, %f251;
	cvt.f64.f32	%fd606, %f252;
	// inline asm
	ld.global.nc.f32 %f239, [%rd65];
	// inline asm
	cvt.f64.f32	%fd607, %f239;
	add.f64 	%fd608, %fd607, 0d3FF0000000000000;
	div.rn.f64 	%fd609, %fd606, %fd608;
	// inline asm
	ld.global.nc.f32 %f240, [%rd289];
	// inline asm
	mul.f32 	%f253, %f275, %f240;
	div.rn.f32 	%f254, %f253, %f19;
	fma.rn.f32 	%f255, %f19, %f250, %f254;
	cvt.f64.f32	%fd610, %f255;
	mul.f64 	%fd611, %fd609, %fd610;
	cvt.rn.f32.f64	%f256, %fd611;
	atom.global.add.f32 	%f257, [%rd288], %f256;
	add.s64 	%rd291, %rd291, 4;
	add.s64 	%rd290, %rd290, 4;
	add.s64 	%rd289, %rd289, 4;
	add.s64 	%rd288, %rd288, 4;
	add.s64 	%rd287, %rd287, 4;
	add.s32 	%r825, %r825, 1;
	setp.lt.s32	%p353, %r825, %r225;
	@%p353 bra 	BB0_372;

BB0_379:
	ld.param.u32 	%r723, [_Z32LargeMarginSoftmaxGradCudaKernelIfEvN10tensorflow16CudaLaunchConfigEPKT_S4_S4_PKfPKiiiiibPfS9_S9_S9_PS2_SA__param_0];
	add.s32 	%r773, %r773, %r71;
	setp.lt.s32	%p354, %r773, %r723;
	@%p354 bra 	BB0_135;

BB0_380:
	ret;
}

	// .globl	_ZN10tensorflow7SetZeroIfEEviPT_
.visible .entry _ZN10tensorflow7SetZeroIfEEviPT_(
	.param .u32 _ZN10tensorflow7SetZeroIfEEviPT__param_0,
	.param .u64 _ZN10tensorflow7SetZeroIfEEviPT__param_1
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<5>;


	ld.param.u32 	%r5, [_ZN10tensorflow7SetZeroIfEEviPT__param_0];
	ld.param.u64 	%rd2, [_ZN10tensorflow7SetZeroIfEEviPT__param_1];
	cvta.to.global.u64 	%rd1, %rd2;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r11, %r6, %r7, %r8;
	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r2, %r9, %r6;
	setp.ge.s32	%p1, %r11, %r5;
	@%p1 bra 	BB1_2;

BB1_1:
	mul.wide.s32 	%rd3, %r11, 4;
	add.s64 	%rd4, %rd1, %rd3;
	mov.u32 	%r10, 0;
	st.global.u32 	[%rd4], %r10;
	add.s32 	%r11, %r11, %r2;
	setp.lt.s32	%p2, %r11, %r5;
	@%p2 bra 	BB1_1;

BB1_2:
	ret;
}

.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
{
	.local .align 8 .b8 	__local_depot2[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<9>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<101>;


	mov.u64 	%rd100, __local_depot2;
	cvta.local.u64 	%SP, %rd100;
	ld.param.f64 	%fd4, [__internal_trig_reduction_slowpathd_param_0];
	ld.param.u64 	%rd37, [__internal_trig_reduction_slowpathd_param_1];
	add.u64 	%rd38, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd38;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd4;
	}
	and.b32  	%r40, %r1, -2147483648;
	shr.u32 	%r3, %r1, 20;
	bfe.u32 	%r4, %r1, 20, 11;
	setp.eq.s32	%p1, %r4, 2047;
	@%p1 bra 	BB2_13;

	add.s32 	%r15, %r4, -1024;
	shr.u32 	%r16, %r15, 6;
	mov.u32 	%r17, 15;
	sub.s32 	%r5, %r17, %r16;
	mov.u32 	%r18, 19;
	sub.s32 	%r19, %r18, %r16;
	mov.u32 	%r20, 18;
	min.s32 	%r6, %r20, %r19;
	mov.u64 	%rd93, 0;
	setp.ge.s32	%p2, %r5, %r6;
	mov.u64 	%rd92, %rd1;
	@%p2 bra 	BB2_4;

	mov.b64 	 %rd41, %fd4;
	shl.b64 	%rd42, %rd41, 11;
	or.b64  	%rd3, %rd42, -9223372036854775808;
	bfe.u32 	%r21, %r1, 20, 11;
	add.s32 	%r22, %r21, -1024;
	shr.u32 	%r23, %r22, 6;
	sub.s32 	%r25, %r17, %r23;
	mul.wide.s32 	%rd43, %r25, 8;
	mov.u64 	%rd44, __cudart_i2opi_d;
	add.s64 	%rd88, %rd44, %rd43;
	mov.u64 	%rd93, 0;
	mov.u64 	%rd92, %rd1;
	mov.u64 	%rd90, %rd1;
	mov.u32 	%r39, %r5;

BB2_3:
	.pragma "nounroll";
	ld.const.u64 	%rd47, [%rd88];
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi, clo, chi;
	mov.b64         {alo,ahi}, %rd47;    
	mov.b64         {blo,bhi}, %rd3;    
	mov.b64         {clo,chi}, %rd93;    
	mad.lo.cc.u32   r0, alo, blo, clo;
	madc.hi.cc.u32  r1, alo, blo, chi;
	madc.hi.u32     r2, alo, bhi,   0;
	mad.lo.cc.u32   r1, alo, bhi,  r1;
	madc.hi.cc.u32  r2, ahi, blo,  r2;
	madc.hi.u32     r3, ahi, bhi,   0;
	mad.lo.cc.u32   r1, ahi, blo,  r1;
	madc.lo.cc.u32  r2, ahi, bhi,  r2;
	addc.u32        r3,  r3,   0;     
	mov.b64         %rd45, {r0,r1};      
	mov.b64         %rd93, {r2,r3};      
	}
	// inline asm
	st.local.u64 	[%rd90], %rd45;
	add.s32 	%r39, %r39, 1;
	sub.s32 	%r26, %r39, %r5;
	mul.wide.s32 	%rd50, %r26, 8;
	add.s64 	%rd90, %rd1, %rd50;
	add.s64 	%rd92, %rd92, 8;
	add.s64 	%rd88, %rd88, 8;
	setp.lt.s32	%p3, %r39, %r6;
	@%p3 bra 	BB2_3;

BB2_4:
	st.local.u64 	[%rd92], %rd93;
	ld.local.u64 	%rd94, [%rd1+16];
	ld.local.u64 	%rd95, [%rd1+24];
	and.b32  	%r9, %r3, 63;
	setp.eq.s32	%p4, %r9, 0;
	@%p4 bra 	BB2_6;

	mov.u32 	%r27, 64;
	sub.s32 	%r28, %r27, %r9;
	shl.b64 	%rd51, %rd95, %r9;
	shr.u64 	%rd52, %rd94, %r28;
	or.b64  	%rd95, %rd51, %rd52;
	shl.b64 	%rd53, %rd94, %r9;
	ld.local.u64 	%rd54, [%rd1+8];
	shr.u64 	%rd55, %rd54, %r28;
	or.b64  	%rd94, %rd55, %rd53;

BB2_6:
	shr.u64 	%rd56, %rd95, 62;
	cvt.u32.u64	%r29, %rd56;
	shr.u64 	%rd57, %rd94, 62;
	shl.b64 	%rd58, %rd95, 2;
	or.b64  	%rd97, %rd58, %rd57;
	shl.b64 	%rd96, %rd94, 2;
	shr.u64 	%rd59, %rd95, 61;
	cvt.u32.u64	%r30, %rd59;
	and.b32  	%r31, %r30, 1;
	add.s32 	%r32, %r31, %r29;
	neg.s32 	%r33, %r32;
	setp.eq.s32	%p5, %r40, 0;
	selp.b32	%r34, %r32, %r33, %p5;
	st.u32 	[%rd37], %r34;
	setp.eq.s32	%p6, %r31, 0;
	@%p6 bra 	BB2_8;

	mov.u64 	%rd63, 0;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd63;
	mov.b64         {a2,a3}, %rd63;
	mov.b64         {b0,b1}, %rd96;
	mov.b64         {b2,b3}, %rd97;
	sub.cc.u32      r0, a0, b0; 
	subc.cc.u32     r1, a1, b1; 
	subc.cc.u32     r2, a2, b2; 
	subc.u32        r3, a3, b3; 
	mov.b64         %rd96, {r0,r1};
	mov.b64         %rd97, {r2,r3};
	}
	// inline asm
	xor.b32  	%r40, %r40, -2147483648;

BB2_8:
	clz.b64 	%r41, %rd97;
	setp.eq.s32	%p7, %r41, 0;
	@%p7 bra 	BB2_10;

	shl.b64 	%rd66, %rd97, %r41;
	mov.u32 	%r35, 64;
	sub.s32 	%r36, %r35, %r41;
	shr.u64 	%rd67, %rd96, %r36;
	or.b64  	%rd97, %rd67, %rd66;

BB2_10:
	mov.u64 	%rd71, -3958705157555305931;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi;
	mov.b64         {alo,ahi}, %rd97;   
	mov.b64         {blo,bhi}, %rd71;   
	mul.lo.u32      r0, alo, blo;    
	mul.hi.u32      r1, alo, blo;    
	mad.lo.cc.u32   r1, alo, bhi, r1;
	madc.hi.u32     r2, alo, bhi,  0;
	mad.lo.cc.u32   r1, ahi, blo, r1;
	madc.hi.cc.u32  r2, ahi, blo, r2;
	madc.hi.u32     r3, ahi, bhi,  0;
	mad.lo.cc.u32   r2, ahi, bhi, r2;
	addc.u32        r3, r3,  0;      
	mov.b64         %rd68, {r0,r1};     
	mov.b64         %rd99, {r2,r3};     
	}
	// inline asm
	setp.lt.s64	%p8, %rd99, 1;
	@%p8 bra 	BB2_12;

	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd68;
	mov.b64         {a2,a3}, %rd99;
	mov.b64         {b0,b1}, %rd68;
	mov.b64         {b2,b3}, %rd99;
	add.cc.u32      r0, a0, b0; 
	addc.cc.u32     r1, a1, b1; 
	addc.cc.u32     r2, a2, b2; 
	addc.u32        r3, a3, b3; 
	mov.b64         %rd72, {r0,r1};
	mov.b64         %rd99, {r2,r3};
	}
	// inline asm
	add.s32 	%r41, %r41, 1;

BB2_12:
	cvt.u64.u32	%rd78, %r40;
	shl.b64 	%rd79, %rd78, 32;
	mov.u32 	%r37, 1022;
	sub.s32 	%r38, %r37, %r41;
	cvt.u64.u32	%rd80, %r38;
	shl.b64 	%rd81, %rd80, 52;
	add.s64 	%rd82, %rd99, 1;
	shr.u64 	%rd83, %rd82, 10;
	add.s64 	%rd84, %rd83, 1;
	shr.u64 	%rd85, %rd84, 1;
	add.s64 	%rd86, %rd85, %rd81;
	or.b64  	%rd87, %rd86, %rd79;
	mov.b64 	 %fd4, %rd87;

BB2_13:
	st.param.f64	[func_retval0+0], %fd4;
	ret;
}

.func  (.param .b64 func_retval0) __internal_accurate_pow(
	.param .b64 __internal_accurate_pow_param_0,
	.param .b64 __internal_accurate_pow_param_1
)
{
	.reg .pred 	%p<9>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<53>;
	.reg .f64 	%fd<138>;


	ld.param.f64 	%fd12, [__internal_accurate_pow_param_0];
	ld.param.f64 	%fd13, [__internal_accurate_pow_param_1];
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd12;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r49, %temp}, %fd12;
	}
	shr.u32 	%r51, %r50, 20;
	setp.ne.s32	%p1, %r51, 0;
	@%p1 bra 	BB3_2;

	mul.f64 	%fd14, %fd12, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r50}, %fd14;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r49, %temp}, %fd14;
	}
	shr.u32 	%r16, %r50, 20;
	add.s32 	%r51, %r16, -54;

BB3_2:
	add.s32 	%r52, %r51, -1023;
	and.b32  	%r17, %r50, -2146435073;
	or.b32  	%r18, %r17, 1072693248;
	mov.b64 	%fd135, {%r49, %r18};
	setp.lt.u32	%p2, %r18, 1073127583;
	@%p2 bra 	BB3_4;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r19, %temp}, %fd135;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r20}, %fd135;
	}
	add.s32 	%r21, %r20, -1048576;
	mov.b64 	%fd135, {%r19, %r21};
	add.s32 	%r52, %r51, -1022;

BB3_4:
	add.f64 	%fd15, %fd135, 0d3FF0000000000000;
	rcp.approx.ftz.f64 	%fd16, %fd15;
	neg.f64 	%fd17, %fd15;
	mov.f64 	%fd18, 0d3FF0000000000000;
	fma.rn.f64 	%fd19, %fd17, %fd16, %fd18;
	fma.rn.f64 	%fd20, %fd19, %fd19, %fd19;
	fma.rn.f64 	%fd21, %fd20, %fd16, %fd16;
	add.f64 	%fd22, %fd135, 0dBFF0000000000000;
	mul.f64 	%fd23, %fd22, %fd21;
	fma.rn.f64 	%fd24, %fd22, %fd21, %fd23;
	mul.f64 	%fd25, %fd24, %fd24;
	mov.f64 	%fd26, 0d3ED0F5D241AD3B5A;
	mov.f64 	%fd27, 0d3EB0F5FF7D2CAFE2;
	fma.rn.f64 	%fd28, %fd27, %fd25, %fd26;
	mov.f64 	%fd29, 0d3EF3B20A75488A3F;
	fma.rn.f64 	%fd30, %fd28, %fd25, %fd29;
	mov.f64 	%fd31, 0d3F1745CDE4FAECD5;
	fma.rn.f64 	%fd32, %fd30, %fd25, %fd31;
	mov.f64 	%fd33, 0d3F3C71C7258A578B;
	fma.rn.f64 	%fd34, %fd32, %fd25, %fd33;
	mov.f64 	%fd35, 0d3F6249249242B910;
	fma.rn.f64 	%fd36, %fd34, %fd25, %fd35;
	mov.f64 	%fd37, 0d3F89999999999DFB;
	fma.rn.f64 	%fd38, %fd36, %fd25, %fd37;
	sub.f64 	%fd39, %fd22, %fd24;
	add.f64 	%fd40, %fd39, %fd39;
	neg.f64 	%fd41, %fd24;
	fma.rn.f64 	%fd42, %fd41, %fd22, %fd40;
	mul.f64 	%fd43, %fd21, %fd42;
	fma.rn.f64 	%fd44, %fd25, %fd38, 0d3FB5555555555555;
	mov.f64 	%fd45, 0d3FB5555555555555;
	sub.f64 	%fd46, %fd45, %fd44;
	fma.rn.f64 	%fd47, %fd25, %fd38, %fd46;
	add.f64 	%fd48, %fd47, 0d0000000000000000;
	add.f64 	%fd49, %fd48, 0dBC46A4CB00B9E7B0;
	add.f64 	%fd50, %fd44, %fd49;
	sub.f64 	%fd51, %fd44, %fd50;
	add.f64 	%fd52, %fd49, %fd51;
	mul.rn.f64 	%fd53, %fd24, %fd24;
	neg.f64 	%fd54, %fd53;
	fma.rn.f64 	%fd55, %fd24, %fd24, %fd54;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r22, %temp}, %fd43;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r23}, %fd43;
	}
	add.s32 	%r24, %r23, 1048576;
	mov.b64 	%fd56, {%r22, %r24};
	fma.rn.f64 	%fd57, %fd24, %fd56, %fd55;
	mul.rn.f64 	%fd58, %fd53, %fd24;
	neg.f64 	%fd59, %fd58;
	fma.rn.f64 	%fd60, %fd53, %fd24, %fd59;
	fma.rn.f64 	%fd61, %fd53, %fd43, %fd60;
	fma.rn.f64 	%fd62, %fd57, %fd24, %fd61;
	mul.rn.f64 	%fd63, %fd50, %fd58;
	neg.f64 	%fd64, %fd63;
	fma.rn.f64 	%fd65, %fd50, %fd58, %fd64;
	fma.rn.f64 	%fd66, %fd50, %fd62, %fd65;
	fma.rn.f64 	%fd67, %fd52, %fd58, %fd66;
	add.f64 	%fd68, %fd63, %fd67;
	sub.f64 	%fd69, %fd63, %fd68;
	add.f64 	%fd70, %fd67, %fd69;
	add.f64 	%fd71, %fd24, %fd68;
	sub.f64 	%fd72, %fd24, %fd71;
	add.f64 	%fd73, %fd68, %fd72;
	add.f64 	%fd74, %fd70, %fd73;
	add.f64 	%fd75, %fd43, %fd74;
	add.f64 	%fd76, %fd71, %fd75;
	sub.f64 	%fd77, %fd71, %fd76;
	add.f64 	%fd78, %fd75, %fd77;
	xor.b32  	%r25, %r52, -2147483648;
	mov.u32 	%r26, 1127219200;
	mov.b64 	%fd79, {%r25, %r26};
	mov.u32 	%r27, -2147483648;
	mov.b64 	%fd80, {%r27, %r26};
	sub.f64 	%fd81, %fd79, %fd80;
	mov.f64 	%fd82, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd83, %fd81, %fd82, %fd76;
	neg.f64 	%fd84, %fd81;
	fma.rn.f64 	%fd85, %fd84, %fd82, %fd83;
	sub.f64 	%fd86, %fd85, %fd76;
	sub.f64 	%fd87, %fd78, %fd86;
	mov.f64 	%fd88, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd89, %fd81, %fd88, %fd87;
	add.f64 	%fd90, %fd83, %fd89;
	sub.f64 	%fd91, %fd83, %fd90;
	add.f64 	%fd92, %fd89, %fd91;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r28}, %fd13;
	}
	add.s32 	%r29, %r28, %r28;
	setp.gt.u32	%p3, %r29, -33554433;
	and.b32  	%r30, %r28, -15728641;
	selp.b32	%r31, %r30, %r28, %p3;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r32, %temp}, %fd13;
	}
	mov.b64 	%fd93, {%r32, %r31};
	mul.rn.f64 	%fd94, %fd90, %fd93;
	neg.f64 	%fd95, %fd94;
	fma.rn.f64 	%fd96, %fd90, %fd93, %fd95;
	fma.rn.f64 	%fd97, %fd92, %fd93, %fd96;
	add.f64 	%fd4, %fd94, %fd97;
	sub.f64 	%fd98, %fd94, %fd4;
	add.f64 	%fd5, %fd97, %fd98;
	mov.f64 	%fd99, 0d4338000000000000;
	mov.f64 	%fd100, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd101, %fd4, %fd100, %fd99;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r13, %temp}, %fd101;
	}
	mov.f64 	%fd102, 0dC338000000000000;
	add.rn.f64 	%fd103, %fd101, %fd102;
	mov.f64 	%fd104, 0dBFE62E42FEFA39EF;
	fma.rn.f64 	%fd105, %fd103, %fd104, %fd4;
	mov.f64 	%fd106, 0dBC7ABC9E3B39803F;
	fma.rn.f64 	%fd107, %fd103, %fd106, %fd105;
	mov.f64 	%fd108, 0d3E928AF3FCA213EA;
	mov.f64 	%fd109, 0d3E5ADE1569CE2BDF;
	fma.rn.f64 	%fd110, %fd109, %fd107, %fd108;
	mov.f64 	%fd111, 0d3EC71DEE62401315;
	fma.rn.f64 	%fd112, %fd110, %fd107, %fd111;
	mov.f64 	%fd113, 0d3EFA01997C89EB71;
	fma.rn.f64 	%fd114, %fd112, %fd107, %fd113;
	mov.f64 	%fd115, 0d3F2A01A014761F65;
	fma.rn.f64 	%fd116, %fd114, %fd107, %fd115;
	mov.f64 	%fd117, 0d3F56C16C1852B7AF;
	fma.rn.f64 	%fd118, %fd116, %fd107, %fd117;
	mov.f64 	%fd119, 0d3F81111111122322;
	fma.rn.f64 	%fd120, %fd118, %fd107, %fd119;
	mov.f64 	%fd121, 0d3FA55555555502A1;
	fma.rn.f64 	%fd122, %fd120, %fd107, %fd121;
	mov.f64 	%fd123, 0d3FC5555555555511;
	fma.rn.f64 	%fd124, %fd122, %fd107, %fd123;
	mov.f64 	%fd125, 0d3FE000000000000B;
	fma.rn.f64 	%fd126, %fd124, %fd107, %fd125;
	fma.rn.f64 	%fd127, %fd126, %fd107, %fd18;
	fma.rn.f64 	%fd128, %fd127, %fd107, %fd18;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r14, %temp}, %fd128;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r15}, %fd128;
	}
	shl.b32 	%r33, %r13, 20;
	add.s32 	%r34, %r15, %r33;
	mov.b64 	%fd136, {%r14, %r34};
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r35}, %fd4;
	}
	mov.b32 	 %f2, %r35;
	abs.f32 	%f1, %f2;
	setp.lt.f32	%p4, %f1, 0f4086232B;
	@%p4 bra 	BB3_7;

	setp.lt.f64	%p5, %fd4, 0d0000000000000000;
	add.f64 	%fd129, %fd4, 0d7FF0000000000000;
	selp.f64	%fd136, 0d0000000000000000, %fd129, %p5;
	setp.geu.f32	%p6, %f1, 0f40874800;
	@%p6 bra 	BB3_7;

	mov.f64 	%fd134, 0d4338000000000000;
	mov.f64 	%fd133, 0d3FF71547652B82FE;
	fma.rn.f64 	%fd132, %fd4, %fd133, %fd134;
	{
	.reg .b32 %temp; 
	mov.b64 	{%r48, %temp}, %fd132;
	}
	shr.u32 	%r36, %r48, 31;
	add.s32 	%r37, %r48, %r36;
	shr.s32 	%r38, %r37, 1;
	shl.b32 	%r39, %r38, 20;
	add.s32 	%r40, %r39, %r15;
	mov.b64 	%fd130, {%r14, %r40};
	sub.s32 	%r41, %r48, %r38;
	shl.b32 	%r42, %r41, 20;
	add.s32 	%r43, %r42, 1072693248;
	mov.u32 	%r44, 0;
	mov.b64 	%fd131, {%r44, %r43};
	mul.f64 	%fd136, %fd130, %fd131;

BB3_7:
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r45}, %fd136;
	}
	and.b32  	%r46, %r45, 2147483647;
	setp.ne.s32	%p7, %r46, 2146435072;
	@%p7 bra 	BB3_9;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r47, %temp}, %fd136;
	}
	setp.eq.s32	%p8, %r47, 0;
	@%p8 bra 	BB3_10;

BB3_9:
	fma.rn.f64 	%fd136, %fd136, %fd5, %fd136;

BB3_10:
	st.param.f64	[func_retval0+0], %fd136;
	ret;
}


